{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import metrics\n",
    "from pytorch_metric_learning import losses\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      outputscore  isVarchar_1  isDate_1  isInt2_1  isInt4_1  isNumeric_1  \\\n",
      "0        0.076672            0         0         0         1            0   \n",
      "1        0.017240            0         0         0         1            0   \n",
      "2        0.156890            0         0         0         1            0   \n",
      "3        0.355420            0         0         0         1            0   \n",
      "4        0.354546            0         0         0         1            0   \n",
      "...           ...          ...       ...       ...       ...          ...   \n",
      "1417     0.003423            1         0         0         0            0   \n",
      "1418     0.035458            1         0         0         0            0   \n",
      "1419     0.016216            0         0         0         1            0   \n",
      "1420     0.042210            1         0         0         0            0   \n",
      "1421     0.016216            0         0         0         1            0   \n",
      "\n",
      "      isOther1  size_1  size_2  isVarchar_2  isDate_2  isInt2_2  isInt4_2  \\\n",
      "0            0      10      13            0         1         0         0   \n",
      "1            0      10      13            0         1         0         0   \n",
      "2            0      10      13            0         1         0         0   \n",
      "3            0      10      13            0         1         0         0   \n",
      "4            0      10      13            0         1         0         0   \n",
      "...        ...     ...     ...          ...       ...       ...       ...   \n",
      "1417         0      50     100            1         0         0         0   \n",
      "1418         0      50     100            1         0         0         0   \n",
      "1419         0      10     100            1         0         0         0   \n",
      "1420         0      50     100            1         0         0         0   \n",
      "1421         0      10     100            1         0         0         0   \n",
      "\n",
      "      isNumeric_2  isOther2  \n",
      "0               0         0  \n",
      "1               0         0  \n",
      "2               0         0  \n",
      "3               0         0  \n",
      "4               0         0  \n",
      "...           ...       ...  \n",
      "1417            0         0  \n",
      "1418            0         0  \n",
      "1419            0         0  \n",
      "1420            0         0  \n",
      "1421            0         0  \n",
      "\n",
      "[14931 rows x 15 columns]\n"
     ]
    }
   ],
   "source": [
    "X=pd.read_excel('cms/'+str(1)+'_encoded_train_cms.xlsx').drop(columns=['Unnamed: 0'])\n",
    "val=pd.read_excel('cms/'+str(1)+'_encoded_validation_cms.xlsx').drop(columns=['Unnamed: 0'])\n",
    "vlabel=pd.read_csv('cms/'+str(1)+'_cmsvalidationlabel.csv')['0']\n",
    "y=pd.read_csv('cms/'+str(1)+'_cmstrainlabel.csv')['0']\n",
    "testlabel=pd.read_csv('cms/'+str(1)+'_cmstestlabel.csv')['0']\n",
    "test=pd.read_excel('cms/'+str(1)+'_encoded_test_cms.xlsx').reindex(columns=X.columns)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       outputscore  isVarchar_1  isDate_1  isInt2_1  isInt4_1  isNumeric_1  \\\n",
      "0         0.014911            0         0         0         1            0   \n",
      "1         0.007701            0         0         0         1            0   \n",
      "2         0.025479            0         0         0         1            0   \n",
      "3         0.016071            0         0         0         1            0   \n",
      "4         0.017303            0         0         0         1            0   \n",
      "...            ...          ...       ...       ...       ...          ...   \n",
      "39811     0.003612            1         0         0         0            0   \n",
      "39812     0.003148            1         0         0         0            0   \n",
      "39813     0.000213            0         0         0         1            0   \n",
      "39814     0.002696            1         0         0         0            0   \n",
      "39815     0.000213            0         0         0         1            0   \n",
      "\n",
      "       isOther1  size_1  size_2  isVarchar_2  isDate_2  isInt2_2  isInt4_2  \\\n",
      "0             0      10      50            1         0         0         0   \n",
      "1             0      10      50            1         0         0         0   \n",
      "2             0      10      50            1         0         0         0   \n",
      "3             0      10      50            1         0         0         0   \n",
      "4             0      10      50            1         0         0         0   \n",
      "...         ...     ...     ...          ...       ...       ...       ...   \n",
      "39811         0      50      22            0         1         0         0   \n",
      "39812         0      50      22            0         1         0         0   \n",
      "39813         0      10      22            0         1         0         0   \n",
      "39814         0      50      22            0         1         0         0   \n",
      "39815         0      10      22            0         1         0         0   \n",
      "\n",
      "       isNumeric_2  isOther2  \n",
      "0                0         0  \n",
      "1                0         0  \n",
      "2                0         0  \n",
      "3                0         0  \n",
      "4                0         0  \n",
      "...            ...       ...  \n",
      "39811            0         0  \n",
      "39812            0         0  \n",
      "39813            0         0  \n",
      "39814            0         0  \n",
      "39815            0         0  \n",
      "\n",
      "[39816 rows x 15 columns]\n"
     ]
    }
   ],
   "source": [
    "X=pd.read_excel('mimic/'+str(2)+'_mimictrainEncoded.xlsx').drop(columns=['Unnamed: 0'])\n",
    "val=pd.read_excel('mimic/'+str(2)+'_mimicvalidationEncoded.xlsx').reindex(columns=X.columns)\n",
    "vlabel=pd.read_csv('mimic/'+str(2)+'_mimicvalidationlabel.csv')['0']\n",
    "y=pd.read_csv('mimic/'+str(2)+'_mimictrainlabel.csv')['0']\n",
    "testlabel=pd.read_csv('mimic/'+str(2)+'_mimictestlabel.csv')['0']\n",
    "test=pd.read_excel('mimic/'+str(2)+'_mimictestEncoded.xlsx').reindex(columns=X.columns)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=pd.read_csv('thalia/unseen_split/'+str(0)+'train_encode.csv').drop(columns=['Unnamed: 0'])\n",
    "val=pd.read_csv('thalia/unseen_split/'+str(0)+'val_encode.csv').reindex(columns=X.columns)\n",
    "vlabel=pd.read_csv('thalia/unseen_split/'+str(0)+'val_label.csv')['label']\n",
    "y=pd.read_csv('thalia/unseen_split/'+str(0)+'train_label.csv')['label']\n",
    "testlabel=pd.read_csv('thalia/unseen_split/'+str(0)+'test_label.csv')['label']\n",
    "test=pd.read_csv('thalia/unseen_split/'+str(0)+'test_encode.csv').reindex(columns=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>primary key_x</th>\n",
       "      <th>foreign key_x</th>\n",
       "      <th>size_x</th>\n",
       "      <th>primary key_y</th>\n",
       "      <th>foreign key_y</th>\n",
       "      <th>size_y</th>\n",
       "      <th>YYYY_x</th>\n",
       "      <th>array_x</th>\n",
       "      <th>boolean_x</th>\n",
       "      <th>integer_x</th>\n",
       "      <th>string_x</th>\n",
       "      <th>array_y</th>\n",
       "      <th>boolean_y</th>\n",
       "      <th>float_y</th>\n",
       "      <th>integer_y</th>\n",
       "      <th>string_y</th>\n",
       "      <th>timestamp_y</th>\n",
       "      <th>outputscore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.005281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.004012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1744</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>110</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.203724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1745</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.004194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1746</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1747</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.002289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1748</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>300</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.852406</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1749 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      primary key_x  foreign key_x  size_x  primary key_y  foreign key_y  \\\n",
       "0                 0              0      13              0              1   \n",
       "1                 0              0      13              0              1   \n",
       "2                 0              0      13              0              0   \n",
       "3                 0              0      13              0              0   \n",
       "4                 0              0      13              0              0   \n",
       "...             ...            ...     ...            ...            ...   \n",
       "1744              1              1      30              1              0   \n",
       "1745              1              1      30              0              0   \n",
       "1746              1              1      30              0              0   \n",
       "1747              1              1      30              0              0   \n",
       "1748              1              1      30              1              0   \n",
       "\n",
       "      size_y  YYYY_x  array_x  boolean_x  integer_x  string_x  array_y  \\\n",
       "0         20       1        0          0          0         0        0   \n",
       "1        200       1        0          0          0         0        0   \n",
       "2         15       1        0          0          0         0        0   \n",
       "3         13       1        0          0          0         0        0   \n",
       "4         13       1        0          0          0         0        0   \n",
       "...      ...     ...      ...        ...        ...       ...      ...   \n",
       "1744     110       0        0          0          0         1        0   \n",
       "1745       5       0        0          0          0         1        0   \n",
       "1746      13       0        0          0          0         1        0   \n",
       "1747      13       0        0          0          0         1        0   \n",
       "1748     300       0        0          0          0         1        0   \n",
       "\n",
       "      boolean_y  float_y  integer_y  string_y  timestamp_y  outputscore  \n",
       "0             0        0          1         0            0     0.005281  \n",
       "1             0        0          1         0            0     0.004012  \n",
       "2             0        0          0         1            0     0.001581  \n",
       "3             0        0          0         0            1     0.000055  \n",
       "4             0        0          0         0            1     0.000023  \n",
       "...         ...      ...        ...       ...          ...          ...  \n",
       "1744          0        0          1         0            0     0.203724  \n",
       "1745          0        1          0         0            0     0.004194  \n",
       "1746          0        0          0         0            1     0.000132  \n",
       "1747          0        0          0         0            1     0.002289  \n",
       "1748          0        0          1         0            0     0.852406  \n",
       "\n",
       "[1749 rows x 18 columns]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=pd.read_csv('imbd/unseen_split/'+str(3)+'_imdb_train_encode.csv')\n",
    "X=X.loc[:,~X.columns.str.match(\"Unnamed\")]\n",
    "val=pd.read_csv('imbd/unseen_split/'+str(3)+'_imdb_val_encode.csv').reindex(columns=X.columns)\n",
    "vlabel=pd.read_csv('imbd/unseen_split/'+str(3)+'_imdb_val_label.csv')['label']\n",
    "y=pd.read_csv('imbd/unseen_split/'+str(3)+'_imdb_train_label.csv')['label']\n",
    "testlabel=pd.read_csv('imbd/unseen_split/'+str(3)+'_imdb_test_label.csv')['label']\n",
    "test=pd.read_csv('imbd/unseen_split/'+str(3)+'_imdb_test_encode.csv').reindex(columns=X.columns)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Double check your datasets carefully, if exist discrpency, fix them.\n",
    "test = test.fillna(0)\n",
    "val=val.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "#         reader = pd.read_excel(fname)\n",
    "        all_data = []\n",
    "        for i in range(x.shape[0]):\n",
    "            feature = torch.tensor(x.iloc[i].to_list(),dtype=torch.float32).to('cpu')\n",
    "            class_n = torch.tensor(y[i],dtype=torch.long).to('cpu')\n",
    "            data = {\n",
    "                'features': feature,\n",
    "                'class_n': int(class_n),\n",
    "            }\n",
    "            all_data.append(data)\n",
    "        self.data = all_data\n",
    "        print('finished data prepration')\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished data prepration\n",
      "finished data prepration\n",
      "finished data prepration\n"
     ]
    }
   ],
   "source": [
    "train = X\n",
    "val = val\n",
    "test = test\n",
    "\n",
    "\"\"\"Here you need input your real label, the lables shown here are just test the model\"\"\"\n",
    "train_y = np.array(y)\n",
    "val_y =  np.array(vlabel)\n",
    "test_y = np.array(testlabel)\n",
    "\n",
    "\n",
    "train = Dataset(train, train_y)\n",
    "val = Dataset(val, val_y)\n",
    "test = Dataset(test, test_y)\n",
    "\n",
    "# Load training dataset labels\n",
    "temp = [t['class_n'] for t in train]\n",
    "# Count different classes sample number\n",
    "class_sample_count = np.array([len(np.where(temp == t)[0]) for t in np.unique(temp)])\n",
    "# Calculate weights for different classes\n",
    "weight_temp = 1. / class_sample_count\n",
    "# Apply weights to each sample\n",
    "samples_weight = np.array([weight_temp[t] for t in temp])\n",
    "samples_weight = torch.from_numpy(samples_weight)\n",
    "# Build sampler for dataloader\n",
    "sampler = torch.utils.data.sampler.WeightedRandomSampler(samples_weight.type('torch.DoubleTensor'), len(samples_weight))\n",
    "\n",
    "train_x = DataLoader(dataset=train, batch_size=64, sampler = sampler)\n",
    "val_x = DataLoader(dataset=val, batch_size=64, shuffle=False)\n",
    "test_x = DataLoader(dataset=test, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _reset_params(model, initializer):\n",
    "    for child in model.children():\n",
    "        for p in child.parameters():\n",
    "            if p.requires_grad:\n",
    "                if initializer == 'uniform':\n",
    "                    stdv = 1. / math.sqrt(p.shape[0])\n",
    "                    torch.nn.init.uniform_(p, a=-stdv, b=stdv)\n",
    "                elif initializer == 'normal':\n",
    "                    torch.nn.init.normal_(p)\n",
    "                elif initializer == 'dirac':\n",
    "                    torch.nn.init.dirac_(p, groups=1)\n",
    "                elif initializer == 'orthogonal':\n",
    "                    torch.nn.init.orthogonal_(p,gain=1)\n",
    "                elif initializer =='kaiming':\n",
    "                    torch.nn.init.kaiming_uniform_(p,a=0,mode='fan_in',nonlinearity='leaky_relu')\n",
    "                elif initializer == 'ones':\n",
    "                    torch.nn.init.ones_(p)\n",
    "                elif initializer=='xavier_normal':\n",
    "                    torch.nn.init.xavier_normal_(p,gain=1.0)\n",
    "                elif initializer =='xavier_uniform':\n",
    "                    if len(p.shape)>1:\n",
    "                        torch.nn.init.xavier_uniform_(p)\n",
    "                    else:\n",
    "                        stdv = 1. / math.sqrt(p.shape[0])\n",
    "                        torch.nn.init.uniform_(p, a=-stdv, b=stdv)\n",
    "\n",
    "\n",
    "def _evaluate_acc_f1(data_loader, model):\n",
    "    n_correct, n_total = 0, 0\n",
    "    t_targets_all, t_outputs_all = None, None\n",
    "    # switch model to evaluation mode\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for t_batch, t_sample_batched in enumerate(data_loader):\n",
    "            t_inputs = t_sample_batched['features']\n",
    "            t_targets = t_sample_batched['class_n']\n",
    "            t_outputs = model(t_inputs)\n",
    "            n_correct += (torch.argmax(t_outputs, -1) == t_targets).sum().item()\n",
    "            n_total += len(t_outputs)\n",
    "\n",
    "            if t_targets_all is None:\n",
    "                t_targets_all = t_targets\n",
    "                t_outputs_all = t_outputs\n",
    "            else:\n",
    "                t_targets_all = torch.cat((t_targets_all, t_targets), dim=0)\n",
    "                t_outputs_all = torch.cat((t_outputs_all, t_outputs), dim=0)\n",
    "    \n",
    "    acc = n_correct / n_total\n",
    "    f1 = metrics.f1_score(t_targets_all.cpu(), torch.argmax(t_outputs_all, -1).cpu(), labels=[0, 1], average='macro')\n",
    "    report = classification_report(t_targets_all.cpu(), torch.argmax(t_outputs_all, -1).cpu(), target_names=['class0','class1'], output_dict=True, digits = 4)\n",
    "    return acc, f1, report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 151.3744, acc: 0.5156\n",
      "loss: 132.9805, acc: 0.5234\n",
      "loss: 129.0620, acc: 0.5156\n",
      "loss: 130.9904, acc: 0.5117\n",
      "loss: 131.2986, acc: 0.4781\n",
      "loss: 124.0195, acc: 0.4870\n",
      "loss: 117.1708, acc: 0.5045\n",
      "loss: 115.5618, acc: 0.4980\n",
      "loss: 111.2968, acc: 0.5069\n",
      "loss: 107.6440, acc: 0.5109\n",
      "loss: 103.9382, acc: 0.5128\n",
      "loss: 100.0029, acc: 0.5091\n",
      "loss: 96.7806, acc: 0.5024\n",
      "loss: 92.1061, acc: 0.5078\n",
      "loss: 88.9169, acc: 0.5062\n",
      "loss: 85.4714, acc: 0.4990\n",
      "loss: 82.0202, acc: 0.4963\n",
      "loss: 78.3920, acc: 0.4878\n",
      "loss: 74.4904, acc: 0.4827\n",
      "loss: 70.8419, acc: 0.4922\n",
      "loss: 67.5727, acc: 0.4985\n",
      "loss: 64.6231, acc: 0.4972\n",
      "loss: 61.9716, acc: 0.4986\n",
      "loss: 59.4519, acc: 0.5078\n",
      "loss: 57.1446, acc: 0.5131\n",
      "loss: 55.0062, acc: 0.5108\n",
      "loss: 53.0591, acc: 0.5133\n",
      "loss: 52.4576, acc: 0.5129\n",
      "val_report: {'precision': 0.024793388429752067, 'recall': 0.75, 'f1-score': 0.048, 'support': 4}\n",
      "\n",
      "Saving model\n",
      "loss: 50.7541, acc: 0.5174\n",
      "loss: 49.1361, acc: 0.5173\n",
      "loss: 47.6135, acc: 0.5245\n",
      "loss: 46.1523, acc: 0.5247\n",
      "loss: 44.7830, acc: 0.5288\n",
      "loss: 43.4875, acc: 0.5312\n",
      "loss: 42.2791, acc: 0.5294\n",
      "loss: 41.1768, acc: 0.5303\n",
      "loss: 40.1460, acc: 0.5265\n",
      "loss: 39.1889, acc: 0.5295\n",
      "loss: 38.1960, acc: 0.5344\n",
      "loss: 37.2553, acc: 0.5368\n",
      "loss: 36.3662, acc: 0.5378\n",
      "loss: 35.5157, acc: 0.5380\n",
      "loss: 34.7039, acc: 0.5415\n",
      "loss: 33.9272, acc: 0.5445\n",
      "loss: 33.1980, acc: 0.5411\n",
      "loss: 32.4957, acc: 0.5433\n",
      "loss: 31.8488, acc: 0.5444\n",
      "loss: 31.2040, acc: 0.5454\n",
      "loss: 30.5928, acc: 0.5470\n",
      "loss: 29.9933, acc: 0.5486\n",
      "loss: 29.4240, acc: 0.5505\n",
      "loss: 28.8840, acc: 0.5507\n",
      "loss: 28.3809, acc: 0.5536\n",
      "loss: 27.8747, acc: 0.5561\n",
      "loss: 27.3945, acc: 0.5574\n",
      "loss: 27.2326, acc: 0.5589\n",
      "val_report: {'precision': 0.030927835051546393, 'recall': 0.75, 'f1-score': 0.05940594059405941, 'support': 4}\n",
      "\n",
      "Saving model\n",
      "loss: 26.7790, acc: 0.5604\n",
      "loss: 26.3281, acc: 0.5629\n",
      "loss: 25.9238, acc: 0.5615\n",
      "loss: 25.5123, acc: 0.5647\n",
      "loss: 25.1493, acc: 0.5665\n",
      "loss: 24.8033, acc: 0.5662\n",
      "loss: 24.4178, acc: 0.5687\n",
      "loss: 24.0468, acc: 0.5681\n",
      "loss: 23.6882, acc: 0.5714\n",
      "loss: 23.3343, acc: 0.5727\n",
      "loss: 23.0098, acc: 0.5740\n",
      "loss: 22.7489, acc: 0.5722\n",
      "loss: 22.4413, acc: 0.5739\n",
      "loss: 22.1307, acc: 0.5751\n",
      "loss: 21.8264, acc: 0.5765\n",
      "loss: 21.5450, acc: 0.5781\n",
      "loss: 21.2621, acc: 0.5789\n",
      "loss: 20.9932, acc: 0.5802\n",
      "loss: 20.7270, acc: 0.5806\n",
      "loss: 20.4823, acc: 0.5812\n",
      "loss: 20.2233, acc: 0.5820\n",
      "loss: 19.9759, acc: 0.5828\n",
      "loss: 19.7494, acc: 0.5809\n",
      "loss: 19.5622, acc: 0.5822\n",
      "loss: 19.3258, acc: 0.5843\n",
      "loss: 19.1053, acc: 0.5858\n",
      "loss: 18.8992, acc: 0.5859\n",
      "loss: 18.8272, acc: 0.5866\n",
      "val_report: {'precision': 0.03614457831325301, 'recall': 0.75, 'f1-score': 0.0689655172413793, 'support': 4}\n",
      "\n",
      "Saving model\n",
      "loss: 18.6210, acc: 0.5876\n",
      "loss: 18.4235, acc: 0.5888\n",
      "loss: 18.2365, acc: 0.5894\n",
      "loss: 18.0642, acc: 0.5882\n",
      "loss: 17.8710, acc: 0.5895\n",
      "loss: 17.7046, acc: 0.5887\n",
      "loss: 17.5290, acc: 0.5886\n",
      "loss: 17.3862, acc: 0.5897\n",
      "loss: 17.2026, acc: 0.5904\n",
      "loss: 17.0210, acc: 0.5928\n",
      "loss: 16.8469, acc: 0.5944\n",
      "loss: 16.6799, acc: 0.5947\n",
      "loss: 16.5690, acc: 0.5950\n",
      "loss: 16.4417, acc: 0.5955\n",
      "loss: 16.2788, acc: 0.5972\n",
      "loss: 16.1184, acc: 0.5994\n",
      "loss: 15.9605, acc: 0.6016\n",
      "loss: 15.8079, acc: 0.6035\n",
      "loss: 15.6615, acc: 0.6028\n",
      "loss: 15.5215, acc: 0.6027\n",
      "loss: 15.3758, acc: 0.6045\n",
      "loss: 15.2360, acc: 0.6050\n",
      "loss: 15.1427, acc: 0.6047\n",
      "loss: 15.0116, acc: 0.6062\n",
      "loss: 14.8840, acc: 0.6074\n",
      "loss: 14.7700, acc: 0.6090\n",
      "loss: 14.6458, acc: 0.6083\n",
      "loss: 14.6035, acc: 0.6092\n",
      "val_report: {'precision': 0.04285714285714286, 'recall': 0.75, 'f1-score': 0.08108108108108107, 'support': 4}\n",
      "\n",
      "Saving model\n",
      "loss: 14.5015, acc: 0.6096\n",
      "loss: 14.3808, acc: 0.6106\n",
      "loss: 14.2685, acc: 0.6116\n",
      "loss: 14.1496, acc: 0.6129\n",
      "loss: 14.0301, acc: 0.6150\n",
      "loss: 13.9166, acc: 0.6164\n",
      "loss: 13.8384, acc: 0.6150\n",
      "loss: 13.7517, acc: 0.6152\n",
      "loss: 13.6438, acc: 0.6163\n",
      "loss: 13.5358, acc: 0.6172\n",
      "loss: 13.4341, acc: 0.6182\n",
      "loss: 13.3338, acc: 0.6198\n",
      "loss: 13.2487, acc: 0.6182\n",
      "loss: 13.1549, acc: 0.6189\n",
      "loss: 13.0646, acc: 0.6200\n",
      "loss: 12.9797, acc: 0.6203\n",
      "loss: 12.8957, acc: 0.6191\n",
      "loss: 12.8160, acc: 0.6195\n",
      "loss: 12.7263, acc: 0.6209\n",
      "loss: 12.6519, acc: 0.6219\n",
      "loss: 12.5740, acc: 0.6217\n",
      "loss: 12.4979, acc: 0.6227\n",
      "loss: 12.4323, acc: 0.6221\n",
      "loss: 12.3505, acc: 0.6234\n",
      "loss: 12.2727, acc: 0.6244\n",
      "loss: 12.1883, acc: 0.6247\n",
      "loss: 12.1081, acc: 0.6255\n",
      "loss: 12.0796, acc: 0.6263\n",
      "val_report: {'precision': 0.05172413793103448, 'recall': 0.75, 'f1-score': 0.0967741935483871, 'support': 4}\n",
      "\n",
      "Saving model\n",
      "loss: 12.0017, acc: 0.6270\n",
      "loss: 11.9397, acc: 0.6257\n",
      "loss: 11.8713, acc: 0.6262\n",
      "loss: 11.7955, acc: 0.6274\n",
      "loss: 11.7170, acc: 0.6278\n",
      "loss: 11.6526, acc: 0.6288\n",
      "loss: 11.5754, acc: 0.6300\n",
      "loss: 11.5073, acc: 0.6294\n",
      "loss: 11.4397, acc: 0.6304\n",
      "loss: 11.3901, acc: 0.6314\n",
      "loss: 11.3249, acc: 0.6309\n",
      "loss: 11.2771, acc: 0.6320\n",
      "loss: 11.2108, acc: 0.6331\n",
      "loss: 11.1395, acc: 0.6340\n",
      "loss: 11.0704, acc: 0.6338\n",
      "loss: 11.0060, acc: 0.6351\n",
      "loss: 10.9372, acc: 0.6365\n",
      "loss: 10.8692, acc: 0.6379\n",
      "loss: 10.8061, acc: 0.6372\n",
      "loss: 10.7586, acc: 0.6379\n",
      "loss: 10.6954, acc: 0.6388\n",
      "loss: 10.6328, acc: 0.6395\n",
      "loss: 10.5901, acc: 0.6388\n",
      "loss: 10.5497, acc: 0.6397\n",
      "loss: 10.4908, acc: 0.6403\n",
      "loss: 10.4593, acc: 0.6392\n",
      "loss: 10.4269, acc: 0.6397\n",
      "loss: 10.4068, acc: 0.6402\n",
      "val_report: {'precision': 0.05454545454545454, 'recall': 0.75, 'f1-score': 0.10169491525423728, 'support': 4}\n",
      "\n",
      "Saving model\n",
      "loss: 10.3523, acc: 0.6410\n",
      "loss: 10.3135, acc: 0.6407\n",
      "loss: 10.2574, acc: 0.6413\n",
      "loss: 10.2114, acc: 0.6417\n",
      "loss: 10.1578, acc: 0.6421\n",
      "loss: 10.1036, acc: 0.6419\n",
      "loss: 10.0702, acc: 0.6427\n",
      "loss: 10.0330, acc: 0.6427\n",
      "loss: 9.9865, acc: 0.6434\n",
      "loss: 9.9447, acc: 0.6433\n",
      "loss: 9.9003, acc: 0.6440\n",
      "loss: 9.8475, acc: 0.6452\n",
      "loss: 9.7942, acc: 0.6466\n",
      "loss: 9.7469, acc: 0.6473\n",
      "loss: 9.6989, acc: 0.6472\n",
      "loss: 9.6538, acc: 0.6479\n",
      "loss: 9.6059, acc: 0.6489\n",
      "loss: 9.5562, acc: 0.6497\n",
      "loss: 9.5090, acc: 0.6497\n",
      "loss: 9.4635, acc: 0.6507\n",
      "loss: 9.4190, acc: 0.6519\n",
      "loss: 9.3740, acc: 0.6522\n",
      "loss: 9.3345, acc: 0.6524\n",
      "loss: 9.3069, acc: 0.6520\n",
      "loss: 9.2869, acc: 0.6509\n",
      "loss: 9.2571, acc: 0.6510\n",
      "loss: 9.2147, acc: 0.6512\n",
      "loss: 9.2006, acc: 0.6516\n",
      "val_report: {'precision': 0.04918032786885246, 'recall': 0.75, 'f1-score': 0.0923076923076923, 'support': 4}\n",
      "loss: 9.1667, acc: 0.6522\n",
      "loss: 9.1236, acc: 0.6520\n",
      "loss: 9.0925, acc: 0.6528\n",
      "loss: 9.0514, acc: 0.6532\n",
      "loss: 9.0083, acc: 0.6533\n",
      "loss: 8.9724, acc: 0.6539\n",
      "loss: 8.9293, acc: 0.6549\n",
      "loss: 8.8861, acc: 0.6559\n",
      "loss: 8.8438, acc: 0.6565\n",
      "loss: 8.8151, acc: 0.6574\n",
      "loss: 8.7773, acc: 0.6574\n",
      "loss: 8.7506, acc: 0.6581\n",
      "loss: 8.7125, acc: 0.6589\n",
      "loss: 8.6753, acc: 0.6593\n",
      "loss: 8.6365, acc: 0.6594\n",
      "loss: 8.6149, acc: 0.6599\n",
      "loss: 8.5745, acc: 0.6614\n",
      "loss: 8.5362, acc: 0.6620\n",
      "loss: 8.5032, acc: 0.6619\n",
      "loss: 8.4790, acc: 0.6626\n",
      "loss: 8.4469, acc: 0.6631\n",
      "loss: 8.4107, acc: 0.6630\n",
      "loss: 8.3821, acc: 0.6636\n",
      "loss: 8.3506, acc: 0.6643\n",
      "loss: 8.3140, acc: 0.6651\n",
      "loss: 8.2911, acc: 0.6639\n",
      "loss: 9.0974, acc: 0.6645\n",
      "loss: 9.0842, acc: 0.6649\n",
      "val_report: {'precision': 0.3, 'recall': 0.75, 'f1-score': 0.4285714285714285, 'support': 4}\n",
      "\n",
      "Saving model\n",
      "loss: 9.0447, acc: 0.6658\n",
      "loss: 9.0053, acc: 0.6668\n",
      "loss: 8.9665, acc: 0.6677\n",
      "loss: 8.9284, acc: 0.6684\n",
      "loss: 8.8925, acc: 0.6679\n",
      "loss: 8.8592, acc: 0.6685\n",
      "loss: 8.8235, acc: 0.6690\n",
      "loss: 8.7867, acc: 0.6693\n",
      "loss: 8.7506, acc: 0.6694\n",
      "loss: 8.7205, acc: 0.6700\n",
      "loss: 8.6947, acc: 0.6705\n",
      "loss: 8.6725, acc: 0.6701\n",
      "loss: 8.6499, acc: 0.6703\n",
      "loss: 8.6152, acc: 0.6704\n",
      "loss: 8.5858, acc: 0.6703\n",
      "loss: 8.5520, acc: 0.6702\n",
      "loss: 8.5250, acc: 0.6711\n",
      "loss: 8.4913, acc: 0.6719\n",
      "loss: 8.4574, acc: 0.6726\n",
      "loss: 8.4235, acc: 0.6733\n",
      "loss: 8.3916, acc: 0.6737\n",
      "loss: 8.3614, acc: 0.6736\n",
      "loss: 8.3296, acc: 0.6742\n",
      "loss: 8.3030, acc: 0.6747\n",
      "loss: 8.2721, acc: 0.6750\n",
      "loss: 8.2442, acc: 0.6748\n",
      "loss: 8.2210, acc: 0.6751\n",
      "loss: 8.2104, acc: 0.6754\n",
      "val_report: {'precision': 0.6666666666666666, 'recall': 0.5, 'f1-score': 0.5714285714285715, 'support': 4}\n",
      "\n",
      "Saving model\n",
      "loss: 8.1986, acc: 0.6751\n",
      "loss: 8.1718, acc: 0.6755\n",
      "loss: 8.1424, acc: 0.6761\n",
      "loss: 8.1121, acc: 0.6760\n",
      "loss: 8.0846, acc: 0.6764\n",
      "loss: 8.0577, acc: 0.6767\n",
      "loss: 8.0284, acc: 0.6769\n",
      "loss: 8.0033, acc: 0.6772\n",
      "loss: 7.9746, acc: 0.6776\n",
      "loss: 7.9457, acc: 0.6780\n",
      "loss: 7.9227, acc: 0.6775\n",
      "loss: 7.8972, acc: 0.6779\n",
      "loss: 7.8779, acc: 0.6782\n",
      "loss: 7.8511, acc: 0.6782\n",
      "loss: 7.8315, acc: 0.6787\n",
      "loss: 7.8044, acc: 0.6784\n",
      "loss: 7.7835, acc: 0.6784\n",
      "loss: 7.7555, acc: 0.6789\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 7.7274, acc: 0.6795\n",
      "loss: 7.6999, acc: 0.6803\n",
      "loss: 7.6811, acc: 0.6808\n",
      "loss: 7.6585, acc: 0.6803\n",
      "loss: 7.6352, acc: 0.6809\n",
      "loss: 7.6098, acc: 0.6814\n",
      "loss: 7.5836, acc: 0.6814\n",
      "loss: 7.5591, acc: 0.6818\n",
      "loss: 7.5326, acc: 0.6824\n",
      "loss: 7.5239, acc: 0.6826\n",
      "val_report: {'precision': 0.42857142857142855, 'recall': 0.75, 'f1-score': 0.5454545454545454, 'support': 4}\n",
      "loss: 7.4976, acc: 0.6835\n",
      "loss: 7.4714, acc: 0.6842\n",
      "loss: 7.4457, acc: 0.6847\n",
      "loss: 7.4209, acc: 0.6851\n",
      "loss: 7.4203, acc: 0.6845\n",
      "loss: 7.3965, acc: 0.6848\n",
      "loss: 7.3725, acc: 0.6851\n",
      "loss: 7.3502, acc: 0.6848\n",
      "loss: 7.3293, acc: 0.6851\n",
      "loss: 7.3069, acc: 0.6856\n",
      "loss: 7.2827, acc: 0.6862\n",
      "loss: 7.2584, acc: 0.6867\n",
      "loss: 7.2377, acc: 0.6870\n",
      "loss: 7.2139, acc: 0.6876\n",
      "loss: 7.1944, acc: 0.6878\n",
      "loss: 7.1855, acc: 0.6875\n",
      "loss: 7.1623, acc: 0.6881\n",
      "loss: 7.1387, acc: 0.6884\n",
      "loss: 7.1152, acc: 0.6889\n",
      "loss: 7.0927, acc: 0.6889\n",
      "loss: 7.0744, acc: 0.6893\n",
      "loss: 7.0512, acc: 0.6901\n",
      "loss: 7.0281, acc: 0.6907\n",
      "loss: 7.0054, acc: 0.6914\n",
      "loss: 6.9828, acc: 0.6920\n",
      "loss: 6.9602, acc: 0.6928\n",
      "loss: 6.9381, acc: 0.6931\n",
      "loss: 6.9308, acc: 0.6933\n",
      "val_report: {'precision': 0.09090909090909091, 'recall': 0.75, 'f1-score': 0.16216216216216214, 'support': 4}\n",
      "loss: 6.9090, acc: 0.6938\n",
      "loss: 6.8873, acc: 0.6944\n",
      "loss: 6.8657, acc: 0.6952\n",
      "loss: 6.8439, acc: 0.6960\n",
      "loss: 6.8225, acc: 0.6967\n",
      "loss: 6.8012, acc: 0.6973\n",
      "loss: 6.7806, acc: 0.6979\n",
      "loss: 6.7597, acc: 0.6983\n",
      "loss: 6.7385, acc: 0.6992\n",
      "loss: 6.7176, acc: 0.6998\n",
      "loss: 6.6967, acc: 0.7006\n",
      "loss: 6.6762, acc: 0.7012\n",
      "loss: 6.6558, acc: 0.7018\n",
      "loss: 6.6361, acc: 0.7022\n",
      "loss: 6.6211, acc: 0.7017\n",
      "loss: 6.6060, acc: 0.7023\n",
      "loss: 6.5876, acc: 0.7029\n",
      "loss: 6.5708, acc: 0.7031\n",
      "loss: 6.5563, acc: 0.7027\n",
      "loss: 6.5409, acc: 0.7031\n",
      "loss: 6.5254, acc: 0.7033\n",
      "loss: 6.5066, acc: 0.7038\n",
      "loss: 6.4892, acc: 0.7038\n",
      "loss: 6.4745, acc: 0.7041\n",
      "loss: 6.4587, acc: 0.7043\n",
      "loss: 6.4453, acc: 0.7042\n",
      "loss: 6.4285, acc: 0.7046\n",
      "loss: 6.4226, acc: 0.7047\n",
      "val_report: {'precision': 0.08333333333333333, 'recall': 0.75, 'f1-score': 0.15, 'support': 4}\n",
      "loss: 6.4044, acc: 0.7055\n",
      "loss: 6.3885, acc: 0.7052\n",
      "loss: 6.3774, acc: 0.7055\n",
      "loss: 6.3631, acc: 0.7053\n",
      "loss: 6.3551, acc: 0.7053\n",
      "loss: 6.3376, acc: 0.7057\n",
      "loss: 6.3327, acc: 0.7053\n",
      "loss: 6.3239, acc: 0.7053\n",
      "loss: 6.3062, acc: 0.7059\n",
      "loss: 6.2882, acc: 0.7067\n",
      "loss: 6.2708, acc: 0.7072\n",
      "loss: 6.2547, acc: 0.7069\n",
      "loss: 6.2470, acc: 0.7071\n",
      "loss: 6.2315, acc: 0.7075\n",
      "loss: 6.2147, acc: 0.7078\n",
      "loss: 6.1986, acc: 0.7081\n",
      "loss: 6.1816, acc: 0.7086\n",
      "loss: 6.1647, acc: 0.7092\n",
      "loss: 6.1508, acc: 0.7096\n",
      "loss: 6.1346, acc: 0.7098\n",
      "loss: 6.1196, acc: 0.7102\n",
      "loss: 6.1051, acc: 0.7106\n",
      "loss: 6.0897, acc: 0.7110\n",
      "loss: 6.0875, acc: 0.7105\n",
      "loss: 6.0772, acc: 0.7110\n",
      "loss: 6.0624, acc: 0.7116\n",
      "loss: 6.0497, acc: 0.7112\n",
      "loss: 6.0468, acc: 0.7113\n",
      "val_report: {'precision': 0.06, 'recall': 0.75, 'f1-score': 0.1111111111111111, 'support': 4}\n",
      "loss: 6.0339, acc: 0.7116\n",
      "loss: 6.0215, acc: 0.7116\n",
      "loss: 6.0113, acc: 0.7118\n",
      "loss: 5.9996, acc: 0.7116\n",
      "loss: 5.9907, acc: 0.7118\n",
      "loss: 5.9766, acc: 0.7123\n",
      "loss: 5.9624, acc: 0.7123\n",
      "loss: 5.9547, acc: 0.7124\n",
      "loss: 5.9410, acc: 0.7121\n",
      "loss: 5.9313, acc: 0.7123\n",
      "loss: 5.9192, acc: 0.7126\n",
      "loss: 5.9037, acc: 0.7131\n",
      "loss: 5.8884, acc: 0.7137\n",
      "loss: 5.8730, acc: 0.7143\n",
      "loss: 5.8577, acc: 0.7148\n",
      "loss: 5.8430, acc: 0.7153\n",
      "loss: 5.8288, acc: 0.7153\n",
      "loss: 5.8162, acc: 0.7157\n",
      "loss: 5.8032, acc: 0.7161\n",
      "loss: 5.7887, acc: 0.7167\n",
      "loss: 5.7742, acc: 0.7170\n",
      "loss: 5.7620, acc: 0.7166\n",
      "loss: 5.7545, acc: 0.7169\n",
      "loss: 5.7422, acc: 0.7173\n",
      "loss: 5.7281, acc: 0.7178\n",
      "loss: 5.7140, acc: 0.7180\n",
      "loss: 5.7053, acc: 0.7178\n",
      "loss: 5.7024, acc: 0.7178\n",
      "val_report: {'precision': 0.07317073170731707, 'recall': 0.75, 'f1-score': 0.13333333333333333, 'support': 4}\n",
      "loss: 5.6887, acc: 0.7183\n",
      "loss: 5.6744, acc: 0.7190\n",
      "loss: 5.6602, acc: 0.7196\n",
      "loss: 5.6462, acc: 0.7202\n",
      "loss: 5.6324, acc: 0.7208\n",
      "loss: 5.6185, acc: 0.7213\n",
      "loss: 5.6090, acc: 0.7213\n",
      "loss: 5.5969, acc: 0.7217\n",
      "loss: 5.5830, acc: 0.7223\n",
      "loss: 5.5695, acc: 0.7229\n",
      "loss: 5.5563, acc: 0.7234\n",
      "loss: 5.5427, acc: 0.7241\n",
      "loss: 5.5298, acc: 0.7245\n",
      "loss: 5.5259, acc: 0.7244\n",
      "loss: 5.5159, acc: 0.7245\n",
      "loss: 5.5094, acc: 0.7244\n",
      "loss: 5.5048, acc: 0.7247\n",
      "loss: 5.4977, acc: 0.7244\n",
      "loss: 5.4889, acc: 0.7247\n",
      "loss: 5.4773, acc: 0.7251\n",
      "loss: 5.4664, acc: 0.7253\n",
      "loss: 5.4584, acc: 0.7253\n",
      "loss: 5.4462, acc: 0.7256\n",
      "loss: 5.4332, acc: 0.7262\n",
      "loss: 5.4206, acc: 0.7267\n",
      "loss: 5.4081, acc: 0.7272\n",
      "loss: 5.3958, acc: 0.7274\n",
      "loss: 5.3920, acc: 0.7276\n",
      "val_report: {'precision': 0.06976744186046512, 'recall': 0.75, 'f1-score': 0.1276595744680851, 'support': 4}\n",
      "loss: 5.3805, acc: 0.7278\n",
      "loss: 5.3691, acc: 0.7278\n",
      "loss: 5.3604, acc: 0.7281\n",
      "loss: 5.3497, acc: 0.7284\n",
      "loss: 5.3401, acc: 0.7283\n",
      "loss: 5.3361, acc: 0.7285\n",
      "loss: 5.3254, acc: 0.7283\n",
      "loss: 5.3161, acc: 0.7287\n",
      "loss: 5.3057, acc: 0.7291\n",
      "loss: 5.3016, acc: 0.7293\n",
      "loss: 5.2916, acc: 0.7290\n",
      "loss: 5.2869, acc: 0.7293\n",
      "loss: 5.2758, acc: 0.7296\n",
      "loss: 5.2727, acc: 0.7291\n",
      "loss: 5.2654, acc: 0.7293\n",
      "loss: 5.2556, acc: 0.7295\n",
      "loss: 5.2508, acc: 0.7294\n",
      "loss: 5.2440, acc: 0.7297\n",
      "loss: 5.2352, acc: 0.7297\n",
      "loss: 5.2270, acc: 0.7298\n",
      "loss: 5.2170, acc: 0.7301\n",
      "loss: 5.2062, acc: 0.7300\n",
      "loss: 5.1959, acc: 0.7305\n",
      "loss: 5.1901, acc: 0.7305\n",
      "loss: 5.1817, acc: 0.7304\n",
      "loss: 5.1767, acc: 0.7307\n",
      "loss: 5.1656, acc: 0.7311\n",
      "loss: 5.1624, acc: 0.7311\n",
      "val_report: {'precision': 0.05263157894736842, 'recall': 0.75, 'f1-score': 0.09836065573770492, 'support': 4}\n",
      "loss: 5.1551, acc: 0.7314\n",
      "loss: 5.1468, acc: 0.7316\n",
      "loss: 5.1398, acc: 0.7315\n",
      "loss: 5.1407, acc: 0.7317\n",
      "loss: 5.1297, acc: 0.7321\n",
      "loss: 5.1187, acc: 0.7326\n",
      "loss: 5.1097, acc: 0.7330\n",
      "loss: 5.1105, acc: 0.7325\n",
      "loss: 5.1052, acc: 0.7327\n",
      "loss: 5.0983, acc: 0.7328\n",
      "loss: 5.0960, acc: 0.7326\n",
      "loss: 5.0875, acc: 0.7329\n",
      "loss: 5.0796, acc: 0.7331\n",
      "loss: 5.0690, acc: 0.7335\n",
      "loss: 5.0588, acc: 0.7339\n",
      "loss: 5.0484, acc: 0.7343\n",
      "loss: 5.0410, acc: 0.7341\n",
      "loss: 5.0347, acc: 0.7342\n",
      "loss: 5.0252, acc: 0.7345\n",
      "loss: 5.0167, acc: 0.7343\n",
      "loss: 5.0134, acc: 0.7346\n",
      "loss: 5.0030, acc: 0.7350\n",
      "loss: 4.9927, acc: 0.7353\n",
      "loss: 4.9827, acc: 0.7354\n",
      "loss: 4.9758, acc: 0.7356\n",
      "loss: 4.9670, acc: 0.7354\n",
      "loss: 4.9619, acc: 0.7357\n",
      "loss: 4.9584, acc: 0.7358\n",
      "val_report: {'precision': 0.06976744186046512, 'recall': 0.75, 'f1-score': 0.1276595744680851, 'support': 4}\n",
      "loss: 4.9484, acc: 0.7363\n",
      "loss: 4.9388, acc: 0.7365\n",
      "loss: 4.9307, acc: 0.7362\n",
      "loss: 4.9281, acc: 0.7364\n",
      "loss: 4.9187, acc: 0.7368\n",
      "loss: 4.9087, acc: 0.7372\n",
      "loss: 4.8986, acc: 0.7377\n",
      "loss: 4.8887, acc: 0.7381\n",
      "loss: 4.8788, acc: 0.7387\n",
      "loss: 4.8689, acc: 0.7392\n",
      "loss: 4.8604, acc: 0.7391\n",
      "loss: 4.8827, acc: 0.7393\n",
      "loss: 4.8751, acc: 0.7389\n",
      "loss: 4.8683, acc: 0.7391\n",
      "loss: 4.8618, acc: 0.7393\n",
      "loss: 4.8521, acc: 0.7397\n",
      "loss: 4.8425, acc: 0.7401\n",
      "loss: 4.8331, acc: 0.7405\n",
      "loss: 4.8238, acc: 0.7406\n",
      "loss: 4.8151, acc: 0.7408\n",
      "loss: 4.8057, acc: 0.7410\n",
      "loss: 4.7966, acc: 0.7411\n",
      "loss: 4.7892, acc: 0.7412\n",
      "loss: 4.7810, acc: 0.7409\n",
      "loss: 4.7749, acc: 0.7412\n",
      "loss: 4.7657, acc: 0.7415\n",
      "loss: 4.7576, acc: 0.7416\n",
      "loss: 4.7549, acc: 0.7417\n",
      "val_report: {'precision': 0.06666666666666667, 'recall': 0.75, 'f1-score': 0.12244897959183675, 'support': 4}\n",
      "loss: 4.7483, acc: 0.7418\n",
      "loss: 4.7405, acc: 0.7418\n",
      "loss: 4.7333, acc: 0.7421\n",
      "loss: 4.7252, acc: 0.7424\n",
      "loss: 4.7209, acc: 0.7423\n",
      "loss: 4.7132, acc: 0.7425\n",
      "loss: 4.7042, acc: 0.7429\n",
      "loss: 4.6953, acc: 0.7433\n",
      "loss: 4.6869, acc: 0.7435\n",
      "loss: 4.6787, acc: 0.7437\n",
      "loss: 4.6702, acc: 0.7438\n",
      "loss: 4.6619, acc: 0.7441\n",
      "loss: 4.6546, acc: 0.7444\n",
      "loss: 4.6497, acc: 0.7439\n",
      "loss: 4.6461, acc: 0.7440\n",
      "loss: 4.6394, acc: 0.7443\n",
      "loss: 4.6310, acc: 0.7446\n",
      "loss: 4.6226, acc: 0.7448\n",
      "loss: 4.6156, acc: 0.7451\n",
      "loss: 4.6073, acc: 0.7453\n",
      "loss: 4.5993, acc: 0.7454\n",
      "loss: 4.5916, acc: 0.7456\n",
      "loss: 4.5833, acc: 0.7459\n",
      "loss: 4.5749, acc: 0.7462\n",
      "loss: 4.5665, acc: 0.7466\n",
      "loss: 4.5583, acc: 0.7469\n",
      "loss: 4.5500, acc: 0.7473\n",
      "loss: 4.5474, acc: 0.7473\n",
      "val_report: {'precision': 0.6, 'recall': 0.75, 'f1-score': 0.6666666666666665, 'support': 4}\n",
      "\n",
      "Saving model\n",
      "loss: 4.5390, acc: 0.7478\n",
      "loss: 4.5308, acc: 0.7480\n",
      "loss: 4.5225, acc: 0.7484\n",
      "loss: 4.5141, acc: 0.7489\n",
      "loss: 4.5059, acc: 0.7492\n",
      "loss: 4.4976, acc: 0.7496\n",
      "loss: 4.4900, acc: 0.7498\n",
      "loss: 4.4830, acc: 0.7497\n",
      "loss: 4.4769, acc: 0.7498\n",
      "loss: 4.4698, acc: 0.7500\n",
      "loss: 4.4617, acc: 0.7504\n",
      "loss: 4.4536, acc: 0.7509\n",
      "loss: 4.4458, acc: 0.7512\n",
      "loss: 4.4397, acc: 0.7509\n",
      "loss: 4.4364, acc: 0.7510\n",
      "loss: 4.4288, acc: 0.7513\n",
      "loss: 4.4213, acc: 0.7513\n",
      "loss: 4.4145, acc: 0.7515\n",
      "loss: 4.4067, acc: 0.7519\n",
      "loss: 4.3992, acc: 0.7522\n",
      "loss: 4.3916, acc: 0.7525\n",
      "loss: 4.3848, acc: 0.7526\n",
      "loss: 4.3801, acc: 0.7524\n",
      "loss: 4.3750, acc: 0.7525\n",
      "loss: 4.3707, acc: 0.7528\n",
      "loss: 4.3664, acc: 0.7527\n",
      "loss: 4.3597, acc: 0.7529\n",
      "loss: 4.3574, acc: 0.7530\n",
      "val_report: {'precision': 0.1875, 'recall': 0.75, 'f1-score': 0.3, 'support': 4}\n",
      "loss: 4.3498, acc: 0.7533\n",
      "loss: 4.3426, acc: 0.7534\n",
      "loss: 4.3372, acc: 0.7536\n",
      "loss: 4.3297, acc: 0.7538\n",
      "loss: 4.3222, acc: 0.7542\n",
      "loss: 4.3147, acc: 0.7545\n",
      "loss: 4.3073, acc: 0.7548\n",
      "loss: 4.2998, acc: 0.7552\n",
      "loss: 4.2924, acc: 0.7555\n",
      "loss: 4.2850, acc: 0.7559\n",
      "loss: 4.2788, acc: 0.7562\n",
      "loss: 4.2728, acc: 0.7561\n",
      "loss: 4.2696, acc: 0.7563\n",
      "loss: 4.2624, acc: 0.7567\n",
      "loss: 4.2553, acc: 0.7569\n",
      "loss: 4.2491, acc: 0.7571\n",
      "loss: 4.2443, acc: 0.7570\n",
      "loss: 4.2377, acc: 0.7573\n",
      "loss: 4.2319, acc: 0.7571\n",
      "loss: 4.2261, acc: 0.7574\n",
      "loss: 4.2209, acc: 0.7575\n",
      "loss: 4.2148, acc: 0.7577\n",
      "loss: 4.2079, acc: 0.7580\n",
      "loss: 4.2023, acc: 0.7583\n",
      "loss: 4.1954, acc: 0.7584\n",
      "loss: 4.1892, acc: 0.7587\n",
      "loss: 4.1826, acc: 0.7590\n",
      "loss: 4.1804, acc: 0.7590\n",
      "val_report: {'precision': 0.6, 'recall': 0.75, 'f1-score': 0.6666666666666665, 'support': 4}\n",
      "loss: 4.1765, acc: 0.7588\n",
      "loss: 4.1754, acc: 0.7589\n",
      "loss: 4.1689, acc: 0.7592\n",
      "loss: 4.1625, acc: 0.7595\n",
      "loss: 4.1574, acc: 0.7595\n",
      "loss: 4.1523, acc: 0.7596\n",
      "loss: 4.1460, acc: 0.7598\n",
      "loss: 4.1391, acc: 0.7602\n",
      "loss: 4.1323, acc: 0.7605\n",
      "loss: 4.1254, acc: 0.7609\n",
      "loss: 4.1195, acc: 0.7610\n",
      "loss: 4.1215, acc: 0.7609\n",
      "loss: 4.1152, acc: 0.7612\n",
      "loss: 4.1121, acc: 0.7610\n",
      "loss: 4.1110, acc: 0.7613\n",
      "loss: 4.1046, acc: 0.7616\n",
      "loss: 4.0979, acc: 0.7620\n",
      "loss: 4.0915, acc: 0.7622\n",
      "loss: 4.0850, acc: 0.7626\n",
      "loss: 4.0784, acc: 0.7628\n",
      "loss: 4.0720, acc: 0.7632\n",
      "loss: 4.0655, acc: 0.7635\n",
      "loss: 4.0599, acc: 0.7637\n",
      "loss: 4.0549, acc: 0.7636\n",
      "loss: 4.0536, acc: 0.7637\n",
      "loss: 4.0478, acc: 0.7638\n",
      "loss: 4.0416, acc: 0.7641\n",
      "loss: 4.0396, acc: 0.7642\n",
      "val_report: {'precision': 0.06818181818181818, 'recall': 0.75, 'f1-score': 0.125, 'support': 4}\n",
      "loss: 4.0337, acc: 0.7644\n",
      "loss: 4.0272, acc: 0.7648\n",
      "loss: 4.0209, acc: 0.7651\n",
      "loss: 4.0145, acc: 0.7655\n",
      "loss: 4.0081, acc: 0.7659\n",
      "loss: 4.0018, acc: 0.7662\n",
      "loss: 3.9958, acc: 0.7664\n",
      "loss: 3.9905, acc: 0.7663\n",
      "loss: 3.9879, acc: 0.7665\n",
      "loss: 3.9816, acc: 0.7668\n",
      "loss: 3.9754, acc: 0.7671\n",
      "loss: 3.9694, acc: 0.7674\n",
      "loss: 3.9677, acc: 0.7672\n",
      "loss: 3.9672, acc: 0.7673\n",
      "loss: 3.9619, acc: 0.7672\n",
      "loss: 3.9601, acc: 0.7673\n",
      "loss: 3.9540, acc: 0.7677\n",
      "loss: 3.9478, acc: 0.7680\n",
      "loss: 3.9420, acc: 0.7682\n",
      "loss: 3.9367, acc: 0.7681\n",
      "loss: 3.9361, acc: 0.7682\n",
      "loss: 3.9302, acc: 0.7684\n",
      "loss: 3.9257, acc: 0.7686\n",
      "loss: 3.9202, acc: 0.7686\n",
      "loss: 3.9163, acc: 0.7687\n",
      "loss: 3.9106, acc: 0.7689\n",
      "loss: 3.9082, acc: 0.7688\n",
      "loss: 3.9066, acc: 0.7688\n",
      "val_report: {'precision': 0.06382978723404255, 'recall': 0.75, 'f1-score': 0.11764705882352941, 'support': 4}\n",
      "loss: 3.9009, acc: 0.7691\n",
      "loss: 3.8956, acc: 0.7693\n",
      "loss: 3.8898, acc: 0.7696\n",
      "loss: 3.8839, acc: 0.7699\n",
      "loss: 3.8781, acc: 0.7701\n",
      "loss: 3.8723, acc: 0.7704\n",
      "loss: 3.8666, acc: 0.7706\n",
      "loss: 3.8614, acc: 0.7705\n",
      "loss: 3.8571, acc: 0.7707\n",
      "loss: 3.8517, acc: 0.7708\n",
      "loss: 3.8459, acc: 0.7712\n",
      "loss: 3.8401, acc: 0.7715\n",
      "loss: 3.8343, acc: 0.7718\n",
      "loss: 3.8286, acc: 0.7721\n",
      "loss: 3.8228, acc: 0.7725\n",
      "loss: 3.8170, acc: 0.7728\n",
      "loss: 3.8113, acc: 0.7732\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 3.8056, acc: 0.7735\n",
      "loss: 3.7999, acc: 0.7738\n",
      "loss: 3.7943, acc: 0.7741\n",
      "loss: 3.7891, acc: 0.7741\n",
      "loss: 3.7858, acc: 0.7743\n",
      "loss: 3.7816, acc: 0.7744\n",
      "loss: 3.7832, acc: 0.7741\n",
      "loss: 3.7780, acc: 0.7744\n",
      "loss: 3.7737, acc: 0.7745\n",
      "loss: 3.7695, acc: 0.7747\n",
      "loss: 3.7678, acc: 0.7747\n",
      "val_report: {'precision': 0.0625, 'recall': 0.75, 'f1-score': 0.11538461538461539, 'support': 4}\n",
      "loss: 3.7636, acc: 0.7748\n",
      "loss: 3.7583, acc: 0.7750\n",
      "loss: 3.7528, acc: 0.7753\n",
      "loss: 3.7475, acc: 0.7755\n",
      "loss: 3.7453, acc: 0.7753\n",
      "loss: 3.7422, acc: 0.7754\n",
      "loss: 3.7378, acc: 0.7755\n",
      "loss: 3.7327, acc: 0.7757\n",
      "loss: 3.7277, acc: 0.7758\n",
      "loss: 3.7235, acc: 0.7760\n",
      "loss: 3.7187, acc: 0.7762\n",
      "loss: 3.7133, acc: 0.7765\n",
      "loss: 3.7080, acc: 0.7768\n",
      "loss: 3.7027, acc: 0.7771\n",
      "loss: 3.6974, acc: 0.7773\n",
      "loss: 3.6921, acc: 0.7777\n",
      "loss: 3.6871, acc: 0.7778\n",
      "loss: 3.6829, acc: 0.7777\n",
      "loss: 3.6796, acc: 0.7779\n",
      "loss: 3.6758, acc: 0.7781\n",
      "loss: 3.6708, acc: 0.7782\n",
      "loss: 3.6685, acc: 0.7783\n",
      "loss: 3.6655, acc: 0.7782\n",
      "loss: 3.6617, acc: 0.7783\n",
      "loss: 3.6583, acc: 0.7785\n",
      "loss: 3.6565, acc: 0.7783\n",
      "loss: 3.6529, acc: 0.7784\n",
      "loss: 3.6512, acc: 0.7785\n",
      "val_report: {'precision': 0.10344827586206896, 'recall': 0.75, 'f1-score': 0.18181818181818182, 'support': 4}\n",
      "loss: 3.6463, acc: 0.7788\n",
      "loss: 3.6411, acc: 0.7791\n",
      "loss: 3.6359, acc: 0.7794\n",
      "loss: 3.6308, acc: 0.7797\n",
      "loss: 3.6260, acc: 0.7799\n",
      "loss: 3.6217, acc: 0.7798\n",
      "loss: 3.6193, acc: 0.7800\n",
      "loss: 3.6148, acc: 0.7802\n",
      "loss: 3.6125, acc: 0.7800\n",
      "loss: 3.6120, acc: 0.7801\n",
      "loss: 3.6070, acc: 0.7804\n",
      "loss: 3.6021, acc: 0.7807\n",
      "loss: 3.5972, acc: 0.7810\n",
      "loss: 3.5922, acc: 0.7812\n",
      "loss: 3.5872, acc: 0.7815\n",
      "loss: 3.5823, acc: 0.7818\n",
      "loss: 3.5774, acc: 0.7821\n",
      "loss: 3.5725, acc: 0.7824\n",
      "loss: 3.5681, acc: 0.7825\n",
      "loss: 3.5659, acc: 0.7823\n",
      "loss: 3.5636, acc: 0.7825\n",
      "loss: 3.5595, acc: 0.7826\n",
      "loss: 3.5546, acc: 0.7829\n",
      "loss: 3.5500, acc: 0.7830\n",
      "loss: 3.5452, acc: 0.7833\n",
      "loss: 3.5405, acc: 0.7836\n",
      "loss: 3.5357, acc: 0.7839\n",
      "loss: 3.5341, acc: 0.7840\n",
      "val_report: {'precision': 0.2727272727272727, 'recall': 0.75, 'f1-score': 0.39999999999999997, 'support': 4}\n",
      "loss: 3.5294, acc: 0.7842\n",
      "loss: 3.5247, acc: 0.7845\n",
      "loss: 3.5202, acc: 0.7846\n",
      "loss: 3.5157, acc: 0.7848\n",
      "loss: 3.5111, acc: 0.7851\n",
      "loss: 3.5077, acc: 0.7852\n",
      "loss: 3.5057, acc: 0.7850\n",
      "loss: 3.5024, acc: 0.7852\n",
      "loss: 3.4991, acc: 0.7853\n",
      "loss: 3.4952, acc: 0.7854\n",
      "loss: 3.4913, acc: 0.7854\n",
      "loss: 3.4877, acc: 0.7856\n",
      "loss: 3.4836, acc: 0.7857\n",
      "loss: 3.4793, acc: 0.7857\n",
      "loss: 3.4764, acc: 0.7858\n",
      "loss: 3.4720, acc: 0.7860\n",
      "loss: 3.4676, acc: 0.7862\n",
      "loss: 3.4633, acc: 0.7864\n",
      "loss: 3.4610, acc: 0.7862\n",
      "loss: 3.4598, acc: 0.7863\n",
      "loss: 3.4558, acc: 0.7865\n",
      "loss: 3.4533, acc: 0.7863\n",
      "loss: 3.4500, acc: 0.7865\n",
      "loss: 3.4473, acc: 0.7866\n",
      "loss: 3.4439, acc: 0.7867\n",
      "loss: 3.4398, acc: 0.7868\n",
      "loss: 3.4356, acc: 0.7870\n",
      "loss: 3.4341, acc: 0.7871\n",
      "val_report: {'precision': 0.08571428571428572, 'recall': 0.75, 'f1-score': 0.15384615384615383, 'support': 4}\n",
      "loss: 3.4297, acc: 0.7873\n",
      "loss: 3.4253, acc: 0.7875\n",
      "loss: 3.4208, acc: 0.7878\n",
      "loss: 3.4164, acc: 0.7881\n",
      "loss: 3.4119, acc: 0.7883\n",
      "loss: 3.4074, acc: 0.7886\n",
      "loss: 3.4030, acc: 0.7888\n",
      "loss: 3.3988, acc: 0.7891\n",
      "loss: 3.3944, acc: 0.7893\n",
      "loss: 3.3902, acc: 0.7896\n",
      "loss: 3.3863, acc: 0.7897\n",
      "loss: 3.3831, acc: 0.7896\n",
      "loss: 3.3789, acc: 0.7899\n",
      "loss: 3.3754, acc: 0.7901\n",
      "loss: 3.3717, acc: 0.7902\n",
      "loss: 3.3681, acc: 0.7900\n",
      "loss: 3.3652, acc: 0.7902\n",
      "loss: 3.3619, acc: 0.7903\n",
      "loss: 3.3590, acc: 0.7904\n",
      "loss: 3.3549, acc: 0.7905\n",
      "loss: 3.3511, acc: 0.7907\n",
      "loss: 3.3468, acc: 0.7909\n",
      "loss: 3.3426, acc: 0.7912\n",
      "loss: 3.3385, acc: 0.7914\n",
      "loss: 3.3360, acc: 0.7913\n",
      "loss: 3.3343, acc: 0.7914\n",
      "loss: 3.3301, acc: 0.7916\n",
      "loss: 3.3287, acc: 0.7917\n",
      "val_report: {'precision': 0.08333333333333333, 'recall': 0.75, 'f1-score': 0.15, 'support': 4}\n",
      "loss: 3.3251, acc: 0.7918\n",
      "loss: 3.3249, acc: 0.7916\n",
      "loss: 3.3250, acc: 0.7916\n",
      "loss: 3.3222, acc: 0.7915\n",
      "loss: 3.3204, acc: 0.7915\n",
      "loss: 3.3195, acc: 0.7916\n",
      "loss: 3.3283, acc: 0.7912\n",
      "loss: 3.3745, acc: 0.7913\n",
      "loss: 3.3774, acc: 0.7911\n",
      "loss: 3.3732, acc: 0.7913\n",
      "loss: 3.3691, acc: 0.7915\n",
      "loss: 3.3650, acc: 0.7916\n",
      "loss: 3.3609, acc: 0.7918\n",
      "loss: 3.3568, acc: 0.7921\n",
      "loss: 3.3528, acc: 0.7922\n",
      "loss: 3.3487, acc: 0.7924\n",
      "loss: 3.3448, acc: 0.7925\n",
      "loss: 3.3407, acc: 0.7928\n",
      "loss: 3.3366, acc: 0.7930\n",
      "loss: 3.3325, acc: 0.7932\n",
      "loss: 3.3284, acc: 0.7935\n",
      "loss: 3.3245, acc: 0.7937\n",
      "loss: 3.3204, acc: 0.7939\n",
      "loss: 3.3164, acc: 0.7941\n",
      "loss: 3.3123, acc: 0.7944\n",
      "loss: 3.3083, acc: 0.7946\n",
      "loss: 3.3043, acc: 0.7948\n",
      "loss: 3.3030, acc: 0.7949\n",
      "val_report: {'precision': 0.3333333333333333, 'recall': 0.75, 'f1-score': 0.46153846153846156, 'support': 4}\n",
      "loss: 3.2989, acc: 0.7952\n",
      "loss: 3.2949, acc: 0.7954\n",
      "loss: 3.2908, acc: 0.7956\n",
      "loss: 3.2870, acc: 0.7958\n",
      "loss: 3.2831, acc: 0.7960\n",
      "loss: 3.2792, acc: 0.7962\n",
      "loss: 3.2754, acc: 0.7963\n",
      "loss: 3.2716, acc: 0.7965\n",
      "loss: 3.2676, acc: 0.7968\n",
      "loss: 3.2638, acc: 0.7969\n",
      "loss: 3.2599, acc: 0.7971\n",
      "loss: 3.2561, acc: 0.7973\n",
      "loss: 3.2523, acc: 0.7975\n",
      "loss: 3.2485, acc: 0.7977\n",
      "loss: 3.2454, acc: 0.7977\n",
      "loss: 3.2434, acc: 0.7976\n",
      "loss: 3.2415, acc: 0.7976\n",
      "loss: 3.2378, acc: 0.7978\n",
      "loss: 3.2340, acc: 0.7980\n",
      "loss: 3.2301, acc: 0.7982\n",
      "loss: 3.2264, acc: 0.7984\n",
      "loss: 3.2225, acc: 0.7986\n",
      "loss: 3.2187, acc: 0.7988\n",
      "loss: 3.2150, acc: 0.7990\n",
      "loss: 3.2112, acc: 0.7992\n",
      "loss: 3.2075, acc: 0.7994\n",
      "loss: 3.2039, acc: 0.7995\n",
      "loss: 3.2028, acc: 0.7995\n",
      "val_report: {'precision': 0.75, 'recall': 0.75, 'f1-score': 0.75, 'support': 4}\n",
      "\n",
      "Saving model\n",
      "loss: 3.2008, acc: 0.7993\n",
      "loss: 3.1981, acc: 0.7995\n",
      "loss: 3.1947, acc: 0.7996\n",
      "loss: 3.1910, acc: 0.7998\n",
      "loss: 3.1872, acc: 0.8000\n",
      "loss: 3.1836, acc: 0.8002\n",
      "loss: 3.1801, acc: 0.8003\n",
      "loss: 3.1766, acc: 0.8004\n",
      "loss: 3.1734, acc: 0.8005\n",
      "loss: 3.1726, acc: 0.8002\n",
      "loss: 3.1730, acc: 0.8003\n",
      "loss: 3.1693, acc: 0.8005\n",
      "loss: 3.1657, acc: 0.8007\n",
      "loss: 3.1620, acc: 0.8009\n",
      "loss: 3.1584, acc: 0.8011\n",
      "loss: 3.1548, acc: 0.8013\n",
      "loss: 3.1512, acc: 0.8015\n",
      "loss: 3.1476, acc: 0.8016\n",
      "loss: 3.1441, acc: 0.8018\n",
      "loss: 3.1407, acc: 0.8019\n",
      "loss: 3.1382, acc: 0.8020\n",
      "loss: 3.1349, acc: 0.8020\n",
      "loss: 3.1314, acc: 0.8022\n",
      "loss: 3.1282, acc: 0.8022\n",
      "loss: 3.1247, acc: 0.8024\n",
      "loss: 3.1211, acc: 0.8026\n",
      "loss: 3.1176, acc: 0.8028\n",
      "loss: 3.1164, acc: 0.8029\n",
      "val_report: {'precision': 0.4444444444444444, 'recall': 1.0, 'f1-score': 0.6153846153846153, 'support': 4}\n",
      "loss: 3.1128, acc: 0.8031\n",
      "loss: 3.1093, acc: 0.8033\n",
      "loss: 3.1058, acc: 0.8036\n",
      "loss: 3.1022, acc: 0.8038\n",
      "loss: 3.0987, acc: 0.8040\n",
      "loss: 3.0952, acc: 0.8042\n",
      "loss: 3.0917, acc: 0.8044\n",
      "loss: 3.0882, acc: 0.8046\n",
      "loss: 3.0847, acc: 0.8048\n",
      "loss: 3.0812, acc: 0.8050\n",
      "loss: 3.0778, acc: 0.8051\n",
      "loss: 3.0745, acc: 0.8053\n",
      "loss: 3.0713, acc: 0.8055\n",
      "loss: 3.0681, acc: 0.8056\n",
      "loss: 3.0646, acc: 0.8058\n",
      "loss: 3.0613, acc: 0.8059\n",
      "loss: 3.0581, acc: 0.8059\n",
      "loss: 3.0576, acc: 0.8060\n",
      "loss: 3.0548, acc: 0.8059\n",
      "loss: 3.0552, acc: 0.8058\n",
      "loss: 3.0519, acc: 0.8060\n",
      "loss: 3.0484, acc: 0.8062\n",
      "loss: 3.0451, acc: 0.8064\n",
      "loss: 3.0416, acc: 0.8067\n",
      "loss: 3.0383, acc: 0.8068\n",
      "loss: 3.0350, acc: 0.8070\n",
      "loss: 3.0316, acc: 0.8072\n",
      "loss: 3.0306, acc: 0.8072\n",
      "val_report: {'precision': 0.6666666666666666, 'recall': 1.0, 'f1-score': 0.8, 'support': 4}\n",
      "\n",
      "Saving model\n",
      "loss: 3.0276, acc: 0.8072\n",
      "loss: 3.0252, acc: 0.8073\n",
      "loss: 3.0225, acc: 0.8074\n",
      "loss: 3.0195, acc: 0.8073\n",
      "loss: 3.0172, acc: 0.8074\n",
      "loss: 3.0144, acc: 0.8075\n",
      "loss: 3.0113, acc: 0.8076\n",
      "loss: 3.0094, acc: 0.8074\n",
      "loss: 3.0107, acc: 0.8074\n",
      "loss: 3.0085, acc: 0.8075\n",
      "loss: 3.0078, acc: 0.8074\n",
      "loss: 3.0059, acc: 0.8075\n",
      "loss: 3.0030, acc: 0.8074\n",
      "loss: 3.0022, acc: 0.8074\n",
      "loss: 3.0009, acc: 0.8073\n",
      "loss: 3.0024, acc: 0.8073\n",
      "loss: 2.9993, acc: 0.8074\n",
      "loss: 2.9961, acc: 0.8075\n",
      "loss: 2.9930, acc: 0.8076\n",
      "loss: 2.9908, acc: 0.8077\n",
      "loss: 2.9877, acc: 0.8079\n",
      "loss: 2.9845, acc: 0.8081\n",
      "loss: 2.9813, acc: 0.8083\n",
      "loss: 2.9780, acc: 0.8085\n",
      "loss: 2.9748, acc: 0.8087\n",
      "loss: 2.9716, acc: 0.8089\n",
      "loss: 2.9684, acc: 0.8091\n",
      "loss: 2.9674, acc: 0.8091\n",
      "val_report: {'precision': 0.4444444444444444, 'recall': 1.0, 'f1-score': 0.6153846153846153, 'support': 4}\n",
      "loss: 2.9642, acc: 0.8093\n",
      "loss: 2.9610, acc: 0.8096\n",
      "loss: 2.9578, acc: 0.8098\n",
      "loss: 2.9548, acc: 0.8099\n",
      "loss: 2.9517, acc: 0.8101\n",
      "loss: 2.9489, acc: 0.8101\n",
      "loss: 2.9479, acc: 0.8100\n",
      "loss: 2.9462, acc: 0.8101\n",
      "loss: 2.9434, acc: 0.8102\n",
      "loss: 2.9402, acc: 0.8104\n",
      "loss: 2.9374, acc: 0.8105\n",
      "loss: 2.9345, acc: 0.8105\n",
      "loss: 2.9320, acc: 0.8106\n",
      "loss: 2.9293, acc: 0.8107\n",
      "loss: 2.9271, acc: 0.8107\n",
      "loss: 2.9258, acc: 0.8107\n",
      "loss: 2.9231, acc: 0.8108\n",
      "loss: 2.9249, acc: 0.8106\n",
      "loss: 2.9235, acc: 0.8107\n",
      "loss: 2.9204, acc: 0.8109\n",
      "loss: 2.9173, acc: 0.8110\n",
      "loss: 2.9144, acc: 0.8112\n",
      "loss: 2.9117, acc: 0.8113\n",
      "loss: 2.9087, acc: 0.8115\n",
      "loss: 2.9057, acc: 0.8116\n",
      "loss: 2.9027, acc: 0.8118\n",
      "loss: 2.8997, acc: 0.8120\n",
      "loss: 2.8987, acc: 0.8121\n",
      "val_report: {'precision': 0.36363636363636365, 'recall': 1.0, 'f1-score': 0.5333333333333333, 'support': 4}\n",
      "loss: 2.8956, acc: 0.8123\n",
      "loss: 2.8926, acc: 0.8125\n",
      "loss: 2.8896, acc: 0.8126\n",
      "loss: 2.8866, acc: 0.8128\n",
      "loss: 2.8837, acc: 0.8130\n",
      "loss: 2.8807, acc: 0.8132\n",
      "loss: 2.8777, acc: 0.8133\n",
      "loss: 2.8748, acc: 0.8135\n",
      "loss: 2.8718, acc: 0.8137\n",
      "loss: 2.8689, acc: 0.8138\n",
      "loss: 2.8660, acc: 0.8140\n",
      "loss: 2.8631, acc: 0.8142\n",
      "loss: 2.8601, acc: 0.8144\n",
      "loss: 2.8572, acc: 0.8146\n",
      "loss: 2.8543, acc: 0.8147\n",
      "loss: 2.8514, acc: 0.8149\n",
      "loss: 2.8485, acc: 0.8151\n",
      "loss: 2.8456, acc: 0.8152\n",
      "loss: 2.8429, acc: 0.8153\n",
      "loss: 2.8410, acc: 0.8154\n",
      "loss: 2.8382, acc: 0.8155\n",
      "loss: 2.8353, acc: 0.8157\n",
      "loss: 2.8324, acc: 0.8159\n",
      "loss: 2.8295, acc: 0.8161\n",
      "loss: 2.8267, acc: 0.8162\n",
      "loss: 2.8238, acc: 0.8164\n",
      "loss: 2.8210, acc: 0.8165\n",
      "loss: 2.8201, acc: 0.8166\n",
      "val_report: {'precision': 0.10256410256410256, 'recall': 1.0, 'f1-score': 0.18604651162790695, 'support': 4}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.8174, acc: 0.8167\n",
      "loss: 2.8155, acc: 0.8164\n",
      "loss: 2.8166, acc: 0.8165\n",
      "loss: 2.8138, acc: 0.8167\n",
      "loss: 2.8110, acc: 0.8168\n",
      "loss: 2.8082, acc: 0.8170\n",
      "loss: 2.8054, acc: 0.8172\n",
      "loss: 2.8026, acc: 0.8174\n",
      "loss: 2.7999, acc: 0.8175\n",
      "loss: 2.7972, acc: 0.8175\n",
      "loss: 2.7951, acc: 0.8176\n",
      "loss: 2.7923, acc: 0.8178\n",
      "loss: 2.7895, acc: 0.8179\n",
      "loss: 2.7867, acc: 0.8181\n",
      "loss: 2.7840, acc: 0.8183\n",
      "loss: 2.7812, acc: 0.8184\n",
      "loss: 2.7785, acc: 0.8186\n",
      "loss: 2.7760, acc: 0.8187\n",
      "loss: 2.7737, acc: 0.8185\n",
      "loss: 2.7732, acc: 0.8185\n",
      "loss: 2.7708, acc: 0.8186\n",
      "loss: 2.7685, acc: 0.8185\n",
      "loss: 2.7672, acc: 0.8186\n",
      "loss: 2.7649, acc: 0.8187\n",
      "loss: 2.7624, acc: 0.8188\n",
      "loss: 2.7635, acc: 0.8186\n",
      "loss: 2.7621, acc: 0.8186\n",
      "loss: 2.7612, acc: 0.8187\n",
      "val_report: {'precision': 0.18181818181818182, 'recall': 1.0, 'f1-score': 0.3076923076923077, 'support': 4}\n",
      "loss: 2.7586, acc: 0.8187\n",
      "loss: 2.7562, acc: 0.8187\n",
      "loss: 2.7538, acc: 0.8189\n",
      "loss: 2.7514, acc: 0.8190\n",
      "loss: 2.7494, acc: 0.8191\n",
      "loss: 2.7491, acc: 0.8189\n",
      "loss: 2.7488, acc: 0.8190\n",
      "loss: 2.7463, acc: 0.8191\n",
      "loss: 2.7436, acc: 0.8192\n",
      "loss: 2.7410, acc: 0.8194\n",
      "loss: 2.7383, acc: 0.8196\n",
      "loss: 2.7357, acc: 0.8197\n",
      "loss: 2.7331, acc: 0.8199\n",
      "loss: 2.7307, acc: 0.8199\n",
      "loss: 2.7290, acc: 0.8200\n",
      "loss: 2.7265, acc: 0.8201\n",
      "loss: 2.7250, acc: 0.8201\n",
      "loss: 2.7237, acc: 0.8202\n",
      "loss: 2.7211, acc: 0.8203\n",
      "loss: 2.7185, acc: 0.8205\n",
      "loss: 2.7160, acc: 0.8206\n",
      "loss: 2.7136, acc: 0.8207\n",
      "loss: 2.7110, acc: 0.8208\n",
      "loss: 2.7085, acc: 0.8210\n",
      "loss: 2.7059, acc: 0.8212\n",
      "loss: 2.7033, acc: 0.8213\n",
      "loss: 2.7008, acc: 0.8214\n",
      "loss: 2.6999, acc: 0.8215\n",
      "val_report: {'precision': 0.4444444444444444, 'recall': 1.0, 'f1-score': 0.6153846153846153, 'support': 4}\n",
      "loss: 2.6974, acc: 0.8217\n",
      "loss: 2.6948, acc: 0.8218\n",
      "loss: 2.6922, acc: 0.8220\n",
      "loss: 2.6896, acc: 0.8222\n",
      "loss: 2.6870, acc: 0.8224\n",
      "loss: 2.6844, acc: 0.8225\n",
      "loss: 2.6819, acc: 0.8227\n",
      "loss: 2.6794, acc: 0.8228\n",
      "loss: 2.6770, acc: 0.8229\n",
      "loss: 2.6756, acc: 0.8230\n",
      "loss: 2.6736, acc: 0.8229\n",
      "loss: 2.6752, acc: 0.8229\n",
      "loss: 2.6729, acc: 0.8229\n",
      "loss: 2.6707, acc: 0.8230\n",
      "loss: 2.6686, acc: 0.8231\n",
      "loss: 2.6676, acc: 0.8230\n",
      "loss: 2.6671, acc: 0.8230\n",
      "loss: 2.6647, acc: 0.8232\n",
      "loss: 2.6622, acc: 0.8233\n",
      "loss: 2.6598, acc: 0.8234\n",
      "loss: 2.6573, acc: 0.8236\n",
      "loss: 2.6549, acc: 0.8237\n",
      "loss: 2.6524, acc: 0.8239\n",
      "loss: 2.6498, acc: 0.8241\n",
      "loss: 2.6474, acc: 0.8242\n",
      "loss: 2.6449, acc: 0.8244\n",
      "loss: 2.6425, acc: 0.8245\n",
      "loss: 2.6417, acc: 0.8246\n",
      "val_report: {'precision': 0.23529411764705882, 'recall': 1.0, 'f1-score': 0.38095238095238093, 'support': 4}\n",
      "loss: 2.6392, acc: 0.8247\n",
      "loss: 2.6371, acc: 0.8247\n",
      "loss: 2.6353, acc: 0.8248\n",
      "loss: 2.6337, acc: 0.8248\n",
      "loss: 2.6325, acc: 0.8248\n",
      "loss: 2.6307, acc: 0.8249\n",
      "loss: 2.6287, acc: 0.8249\n",
      "loss: 2.6282, acc: 0.8248\n",
      "loss: 2.6275, acc: 0.8247\n",
      "loss: 2.6259, acc: 0.8248\n",
      "loss: 2.6236, acc: 0.8248\n",
      "loss: 2.6218, acc: 0.8249\n",
      "loss: 2.6195, acc: 0.8250\n",
      "loss: 2.6171, acc: 0.8251\n",
      "loss: 2.6147, acc: 0.8253\n",
      "loss: 2.6123, acc: 0.8254\n",
      "loss: 2.6099, acc: 0.8256\n",
      "loss: 2.6075, acc: 0.8257\n",
      "loss: 2.6051, acc: 0.8259\n",
      "loss: 2.6027, acc: 0.8261\n",
      "loss: 2.6003, acc: 0.8262\n",
      "loss: 2.5979, acc: 0.8264\n",
      "loss: 2.5955, acc: 0.8265\n",
      "loss: 2.5932, acc: 0.8267\n",
      "loss: 2.5908, acc: 0.8268\n",
      "loss: 2.5884, acc: 0.8270\n",
      "loss: 2.5861, acc: 0.8272\n",
      "loss: 2.5853, acc: 0.8272\n",
      "val_report: {'precision': 0.5, 'recall': 0.75, 'f1-score': 0.6, 'support': 4}\n",
      "loss: 2.5829, acc: 0.8274\n",
      "loss: 2.5806, acc: 0.8275\n",
      "loss: 2.5783, acc: 0.8277\n",
      "loss: 2.5760, acc: 0.8278\n",
      "loss: 2.5736, acc: 0.8280\n",
      "loss: 2.5713, acc: 0.8281\n",
      "loss: 2.5690, acc: 0.8283\n",
      "loss: 2.5666, acc: 0.8284\n",
      "loss: 2.5643, acc: 0.8286\n",
      "loss: 2.5620, acc: 0.8287\n",
      "loss: 2.5597, acc: 0.8289\n",
      "loss: 2.5574, acc: 0.8290\n",
      "loss: 2.5556, acc: 0.8291\n",
      "loss: 2.5573, acc: 0.8289\n",
      "loss: 2.6099, acc: 0.8288\n",
      "loss: 2.6078, acc: 0.8288\n",
      "loss: 2.6060, acc: 0.8289\n",
      "loss: 2.6037, acc: 0.8290\n",
      "loss: 2.6014, acc: 0.8291\n",
      "loss: 2.5991, acc: 0.8293\n",
      "loss: 2.5968, acc: 0.8294\n",
      "loss: 2.5945, acc: 0.8296\n",
      "loss: 2.5922, acc: 0.8297\n",
      "loss: 2.5899, acc: 0.8298\n",
      "loss: 2.5876, acc: 0.8300\n",
      "loss: 2.5853, acc: 0.8301\n",
      "loss: 2.5831, acc: 0.8303\n",
      "loss: 2.5823, acc: 0.8303\n",
      "val_report: {'precision': 0.6, 'recall': 0.75, 'f1-score': 0.6666666666666665, 'support': 4}\n",
      "loss: 2.5801, acc: 0.8304\n",
      "loss: 2.5778, acc: 0.8305\n",
      "loss: 2.5756, acc: 0.8306\n",
      "loss: 2.5733, acc: 0.8308\n",
      "loss: 2.5711, acc: 0.8309\n",
      "loss: 2.5688, acc: 0.8310\n",
      "loss: 2.5666, acc: 0.8312\n",
      "loss: 2.5643, acc: 0.8313\n",
      "loss: 2.5621, acc: 0.8314\n",
      "loss: 2.5598, acc: 0.8315\n",
      "loss: 2.5576, acc: 0.8317\n",
      "loss: 2.5553, acc: 0.8318\n",
      "loss: 2.5531, acc: 0.8319\n",
      "loss: 2.5508, acc: 0.8321\n",
      "loss: 2.5486, acc: 0.8322\n",
      "loss: 2.5464, acc: 0.8323\n",
      "loss: 2.5441, acc: 0.8325\n",
      "loss: 2.5420, acc: 0.8326\n",
      "loss: 2.5398, acc: 0.8327\n",
      "loss: 2.5379, acc: 0.8327\n",
      "loss: 2.5373, acc: 0.8326\n",
      "loss: 2.5354, acc: 0.8327\n",
      "loss: 2.5336, acc: 0.8328\n",
      "loss: 2.5323, acc: 0.8327\n",
      "loss: 2.5302, acc: 0.8327\n",
      "loss: 2.5288, acc: 0.8328\n",
      "loss: 2.5266, acc: 0.8329\n",
      "loss: 2.5259, acc: 0.8330\n",
      "val_report: {'precision': 0.36363636363636365, 'recall': 1.0, 'f1-score': 0.5333333333333333, 'support': 4}\n",
      "loss: 2.5237, acc: 0.8331\n",
      "loss: 2.5216, acc: 0.8332\n",
      "loss: 2.5194, acc: 0.8333\n",
      "loss: 2.5173, acc: 0.8334\n",
      "loss: 2.5151, acc: 0.8335\n",
      "loss: 2.5129, acc: 0.8337\n",
      "loss: 2.5108, acc: 0.8338\n",
      "loss: 2.5086, acc: 0.8340\n",
      "loss: 2.5065, acc: 0.8341\n",
      "loss: 2.5044, acc: 0.8342\n",
      "loss: 2.5022, acc: 0.8344\n",
      "loss: 2.5000, acc: 0.8345\n",
      "loss: 2.4979, acc: 0.8346\n",
      "loss: 2.4958, acc: 0.8348\n",
      "loss: 2.4937, acc: 0.8349\n",
      "loss: 2.4916, acc: 0.8350\n",
      "loss: 2.4895, acc: 0.8352\n",
      "loss: 2.4874, acc: 0.8353\n",
      "loss: 2.4853, acc: 0.8354\n",
      "loss: 2.4837, acc: 0.8355\n",
      "loss: 2.4825, acc: 0.8354\n",
      "loss: 2.4814, acc: 0.8354\n",
      "loss: 2.4796, acc: 0.8355\n",
      "loss: 2.4775, acc: 0.8356\n",
      "loss: 2.4755, acc: 0.8357\n",
      "loss: 2.4733, acc: 0.8359\n",
      "loss: 2.4713, acc: 0.8360\n",
      "loss: 2.4706, acc: 0.8361\n",
      "val_report: {'precision': 0.5714285714285714, 'recall': 1.0, 'f1-score': 0.7272727272727273, 'support': 4}\n",
      "loss: 2.4687, acc: 0.8361\n",
      "loss: 2.4669, acc: 0.8362\n",
      "loss: 2.4650, acc: 0.8363\n",
      "loss: 2.4629, acc: 0.8364\n",
      "loss: 2.4608, acc: 0.8365\n",
      "loss: 2.4589, acc: 0.8366\n",
      "loss: 2.4568, acc: 0.8368\n",
      "loss: 2.4548, acc: 0.8369\n",
      "loss: 2.4527, acc: 0.8370\n",
      "loss: 2.4507, acc: 0.8371\n",
      "loss: 2.4487, acc: 0.8373\n",
      "loss: 2.4468, acc: 0.8374\n",
      "loss: 2.4460, acc: 0.8373\n",
      "loss: 2.4442, acc: 0.8374\n",
      "loss: 2.4428, acc: 0.8375\n",
      "loss: 2.4419, acc: 0.8373\n",
      "loss: 2.4415, acc: 0.8373\n",
      "loss: 2.4398, acc: 0.8373\n",
      "loss: 2.4380, acc: 0.8374\n",
      "loss: 2.4360, acc: 0.8375\n",
      "loss: 2.4340, acc: 0.8377\n",
      "loss: 2.4320, acc: 0.8378\n",
      "loss: 2.4300, acc: 0.8379\n",
      "loss: 2.4280, acc: 0.8381\n",
      "loss: 2.4260, acc: 0.8382\n",
      "loss: 2.4240, acc: 0.8383\n",
      "loss: 2.4220, acc: 0.8385\n",
      "loss: 2.4213, acc: 0.8385\n",
      "val_report: {'precision': 0.3333333333333333, 'recall': 1.0, 'f1-score': 0.5, 'support': 4}\n",
      "loss: 2.4193, acc: 0.8386\n",
      "loss: 2.4173, acc: 0.8388\n",
      "loss: 2.4154, acc: 0.8389\n",
      "loss: 2.4135, acc: 0.8390\n",
      "loss: 2.4115, acc: 0.8391\n",
      "loss: 2.4095, acc: 0.8393\n",
      "loss: 2.4076, acc: 0.8394\n",
      "loss: 2.4057, acc: 0.8395\n",
      "loss: 2.4039, acc: 0.8394\n",
      "loss: 2.4031, acc: 0.8395\n",
      "loss: 2.4012, acc: 0.8396\n",
      "loss: 2.3992, acc: 0.8397\n",
      "loss: 2.3973, acc: 0.8398\n",
      "loss: 2.3953, acc: 0.8400\n",
      "loss: 2.3934, acc: 0.8401\n",
      "loss: 2.3915, acc: 0.8402\n",
      "loss: 2.3895, acc: 0.8403\n",
      "loss: 2.3877, acc: 0.8404\n",
      "loss: 2.3877, acc: 0.8403\n",
      "loss: 2.3865, acc: 0.8404\n",
      "loss: 2.3845, acc: 0.8405\n",
      "loss: 2.3826, acc: 0.8406\n",
      "loss: 2.3807, acc: 0.8407\n",
      "loss: 2.3789, acc: 0.8408\n",
      "loss: 2.3770, acc: 0.8410\n",
      "loss: 2.3752, acc: 0.8410\n",
      "loss: 2.3745, acc: 0.8409\n",
      "loss: 2.3755, acc: 0.8409\n",
      "val_report: {'precision': 0.5714285714285714, 'recall': 1.0, 'f1-score': 0.7272727272727273, 'support': 4}\n",
      "loss: 2.3745, acc: 0.8407\n",
      "loss: 2.3732, acc: 0.8408\n",
      "loss: 2.3717, acc: 0.8408\n",
      "loss: 2.3698, acc: 0.8409\n",
      "loss: 2.3679, acc: 0.8411\n",
      "loss: 2.3660, acc: 0.8412\n",
      "loss: 2.3641, acc: 0.8413\n",
      "loss: 2.3622, acc: 0.8414\n",
      "loss: 2.3604, acc: 0.8415\n",
      "loss: 2.3585, acc: 0.8417\n",
      "loss: 2.3566, acc: 0.8418\n",
      "loss: 2.3547, acc: 0.8419\n",
      "loss: 2.3528, acc: 0.8421\n",
      "loss: 2.3509, acc: 0.8422\n",
      "loss: 2.3490, acc: 0.8423\n",
      "loss: 2.3471, acc: 0.8424\n",
      "loss: 2.3454, acc: 0.8425\n",
      "loss: 2.3435, acc: 0.8427\n",
      "loss: 2.3417, acc: 0.8428\n",
      "loss: 2.3399, acc: 0.8428\n",
      "loss: 2.3386, acc: 0.8429\n",
      "loss: 2.3367, acc: 0.8430\n",
      "loss: 2.3349, acc: 0.8431\n",
      "loss: 2.3330, acc: 0.8433\n",
      "loss: 2.3312, acc: 0.8434\n",
      "loss: 2.3294, acc: 0.8435\n",
      "loss: 2.3280, acc: 0.8435\n",
      "loss: 2.3274, acc: 0.8435\n",
      "val_report: {'precision': 0.5714285714285714, 'recall': 1.0, 'f1-score': 0.7272727272727273, 'support': 4}\n",
      "loss: 2.3279, acc: 0.8434\n",
      "loss: 2.3270, acc: 0.8434\n",
      "loss: 2.3255, acc: 0.8435\n",
      "loss: 2.3257, acc: 0.8434\n",
      "loss: 2.3245, acc: 0.8435\n",
      "loss: 2.3229, acc: 0.8434\n",
      "loss: 2.3216, acc: 0.8435\n",
      "loss: 2.3199, acc: 0.8435\n",
      "loss: 2.3186, acc: 0.8435\n",
      "loss: 2.3181, acc: 0.8435\n",
      "loss: 2.3163, acc: 0.8436\n",
      "loss: 2.3146, acc: 0.8437\n",
      "loss: 2.3128, acc: 0.8438\n",
      "loss: 2.3111, acc: 0.8439\n",
      "loss: 2.3099, acc: 0.8438\n",
      "loss: 2.3089, acc: 0.8438\n",
      "loss: 2.3075, acc: 0.8439\n",
      "loss: 2.3058, acc: 0.8440\n",
      "loss: 2.3040, acc: 0.8441\n",
      "loss: 2.3022, acc: 0.8442\n",
      "loss: 2.3005, acc: 0.8443\n",
      "loss: 2.2987, acc: 0.8444\n",
      "loss: 2.2970, acc: 0.8445\n",
      "loss: 2.2955, acc: 0.8446\n",
      "loss: 2.2939, acc: 0.8447\n",
      "loss: 2.2921, acc: 0.8448\n",
      "loss: 2.2920, acc: 0.8446\n",
      "loss: 2.2915, acc: 0.8447\n",
      "val_report: {'precision': 0.07692307692307693, 'recall': 1.0, 'f1-score': 0.14285714285714288, 'support': 4}\n",
      "loss: 2.2908, acc: 0.8447\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.2898, acc: 0.8447\n",
      "loss: 2.2884, acc: 0.8446\n",
      "loss: 2.2885, acc: 0.8446\n",
      "loss: 2.2868, acc: 0.8447\n",
      "loss: 2.2850, acc: 0.8448\n",
      "loss: 2.2833, acc: 0.8450\n",
      "loss: 2.2815, acc: 0.8451\n",
      "loss: 2.2798, acc: 0.8452\n",
      "loss: 2.2780, acc: 0.8453\n",
      "loss: 2.2763, acc: 0.8454\n",
      "loss: 2.2745, acc: 0.8455\n",
      "loss: 2.2728, acc: 0.8456\n",
      "loss: 2.2711, acc: 0.8458\n",
      "loss: 2.2694, acc: 0.8459\n",
      "loss: 2.2676, acc: 0.8460\n",
      "loss: 2.2659, acc: 0.8461\n",
      "loss: 2.2642, acc: 0.8462\n",
      "loss: 2.2626, acc: 0.8463\n",
      "loss: 2.2633, acc: 0.8461\n",
      "loss: 2.2625, acc: 0.8462\n",
      "loss: 2.2612, acc: 0.8462\n",
      "loss: 2.2606, acc: 0.8461\n",
      "loss: 2.2597, acc: 0.8461\n",
      "loss: 2.2587, acc: 0.8461\n",
      "loss: 2.2574, acc: 0.8461\n",
      "loss: 2.2558, acc: 0.8462\n",
      "loss: 2.2559, acc: 0.8462\n",
      "val_report: {'precision': 0.5, 'recall': 0.75, 'f1-score': 0.6, 'support': 4}\n",
      "loss: 2.2583, acc: 0.8460\n",
      "loss: 2.2585, acc: 0.8459\n",
      "loss: 2.2575, acc: 0.8458\n",
      "loss: 2.2574, acc: 0.8458\n",
      "loss: 2.2559, acc: 0.8459\n",
      "loss: 2.2543, acc: 0.8460\n",
      "loss: 2.2529, acc: 0.8459\n",
      "loss: 2.2522, acc: 0.8459\n",
      "loss: 2.2505, acc: 0.8460\n",
      "loss: 2.2489, acc: 0.8461\n",
      "loss: 2.2472, acc: 0.8462\n",
      "loss: 2.2455, acc: 0.8463\n",
      "loss: 2.2438, acc: 0.8464\n",
      "loss: 2.2422, acc: 0.8466\n",
      "loss: 2.2405, acc: 0.8467\n",
      "loss: 2.2388, acc: 0.8468\n",
      "loss: 2.2372, acc: 0.8469\n",
      "loss: 2.2356, acc: 0.8470\n",
      "loss: 2.2339, acc: 0.8471\n",
      "loss: 2.2322, acc: 0.8472\n",
      "loss: 2.2305, acc: 0.8474\n",
      "loss: 2.2289, acc: 0.8475\n",
      "loss: 2.2272, acc: 0.8476\n",
      "loss: 2.2256, acc: 0.8477\n",
      "loss: 2.2239, acc: 0.8478\n",
      "loss: 2.2223, acc: 0.8479\n",
      "loss: 2.2206, acc: 0.8480\n",
      "loss: 2.2201, acc: 0.8481\n",
      "val_report: {'precision': 0.5, 'recall': 1.0, 'f1-score': 0.6666666666666666, 'support': 4}\n",
      "loss: 2.2185, acc: 0.8482\n",
      "loss: 2.2169, acc: 0.8483\n",
      "loss: 2.2152, acc: 0.8484\n",
      "loss: 2.2136, acc: 0.8485\n",
      "loss: 2.2120, acc: 0.8486\n",
      "loss: 2.2106, acc: 0.8486\n",
      "loss: 2.2090, acc: 0.8487\n",
      "loss: 2.2076, acc: 0.8487\n",
      "loss: 2.2071, acc: 0.8487\n",
      "loss: 2.2055, acc: 0.8488\n",
      "loss: 2.2040, acc: 0.8489\n",
      "loss: 2.2027, acc: 0.8489\n",
      "loss: 2.2011, acc: 0.8490\n",
      "loss: 2.1995, acc: 0.8491\n",
      "loss: 2.1979, acc: 0.8492\n",
      "loss: 2.1964, acc: 0.8493\n",
      "loss: 2.1948, acc: 0.8494\n",
      "loss: 2.1932, acc: 0.8495\n",
      "loss: 2.1916, acc: 0.8496\n",
      "loss: 2.1900, acc: 0.8498\n",
      "loss: 2.1884, acc: 0.8499\n",
      "loss: 2.1867, acc: 0.8500\n",
      "loss: 2.1852, acc: 0.8501\n",
      "loss: 2.1836, acc: 0.8502\n",
      "loss: 2.1825, acc: 0.8502\n",
      "loss: 2.1809, acc: 0.8503\n",
      "loss: 2.1793, acc: 0.8504\n",
      "loss: 2.1788, acc: 0.8505\n",
      "val_report: {'precision': 0.4, 'recall': 1.0, 'f1-score': 0.5714285714285715, 'support': 4}\n",
      "loss: 2.1772, acc: 0.8506\n",
      "loss: 2.1756, acc: 0.8507\n",
      "loss: 2.1740, acc: 0.8508\n",
      "loss: 2.1724, acc: 0.8509\n",
      "loss: 2.1709, acc: 0.8510\n",
      "loss: 2.1693, acc: 0.8511\n",
      "loss: 2.1677, acc: 0.8512\n",
      "loss: 2.1664, acc: 0.8512\n",
      "loss: 2.1652, acc: 0.8512\n",
      "loss: 2.1641, acc: 0.8513\n",
      "loss: 2.1627, acc: 0.8513\n",
      "loss: 2.1633, acc: 0.8512\n",
      "loss: 2.1635, acc: 0.8512\n",
      "loss: 2.1626, acc: 0.8511\n",
      "loss: 2.1633, acc: 0.8511\n",
      "loss: 2.1620, acc: 0.8512\n",
      "loss: 2.1606, acc: 0.8512\n",
      "loss: 2.1591, acc: 0.8513\n",
      "loss: 2.1576, acc: 0.8514\n",
      "loss: 2.1560, acc: 0.8515\n",
      "loss: 2.1546, acc: 0.8516\n",
      "loss: 2.1530, acc: 0.8517\n",
      "loss: 2.1515, acc: 0.8518\n",
      "loss: 2.1500, acc: 0.8519\n",
      "loss: 2.1485, acc: 0.8520\n",
      "loss: 2.1470, acc: 0.8521\n",
      "loss: 2.1455, acc: 0.8522\n",
      "loss: 2.1450, acc: 0.8522\n",
      "val_report: {'precision': 0.4, 'recall': 1.0, 'f1-score': 0.5714285714285715, 'support': 4}\n",
      "loss: 2.1434, acc: 0.8523\n",
      "loss: 2.1419, acc: 0.8524\n",
      "loss: 2.1404, acc: 0.8525\n",
      "loss: 2.1389, acc: 0.8526\n",
      "loss: 2.1374, acc: 0.8527\n",
      "loss: 2.1359, acc: 0.8528\n",
      "loss: 2.1344, acc: 0.8529\n",
      "loss: 2.1329, acc: 0.8530\n",
      "loss: 2.1315, acc: 0.8531\n",
      "loss: 2.1303, acc: 0.8529\n",
      "loss: 2.1304, acc: 0.8530\n",
      "loss: 2.1289, acc: 0.8530\n",
      "loss: 2.1274, acc: 0.8531\n",
      "loss: 2.1259, acc: 0.8532\n",
      "loss: 2.1244, acc: 0.8533\n",
      "loss: 2.1229, acc: 0.8534\n",
      "loss: 2.1214, acc: 0.8535\n",
      "loss: 2.1199, acc: 0.8536\n",
      "loss: 2.1185, acc: 0.8537\n",
      "loss: 2.1170, acc: 0.8538\n",
      "loss: 2.1155, acc: 0.8539\n",
      "loss: 2.1141, acc: 0.8540\n",
      "loss: 2.1126, acc: 0.8541\n",
      "loss: 2.1112, acc: 0.8542\n",
      "loss: 2.1097, acc: 0.8543\n",
      "loss: 2.1082, acc: 0.8544\n",
      "loss: 2.1069, acc: 0.8544\n",
      "loss: 2.1065, acc: 0.8544\n",
      "val_report: {'precision': 0.3076923076923077, 'recall': 1.0, 'f1-score': 0.47058823529411764, 'support': 4}\n",
      "loss: 2.1051, acc: 0.8545\n",
      "loss: 2.1036, acc: 0.8546\n",
      "loss: 2.1021, acc: 0.8547\n",
      "loss: 2.1007, acc: 0.8548\n",
      "loss: 2.0992, acc: 0.8549\n",
      "loss: 2.0978, acc: 0.8550\n",
      "loss: 2.0963, acc: 0.8551\n",
      "loss: 2.0949, acc: 0.8552\n",
      "loss: 2.0934, acc: 0.8553\n",
      "loss: 2.0925, acc: 0.8551\n",
      "loss: 2.0935, acc: 0.8551\n",
      "loss: 2.0921, acc: 0.8552\n",
      "loss: 2.0907, acc: 0.8553\n",
      "loss: 2.0892, acc: 0.8554\n",
      "loss: 2.0878, acc: 0.8555\n",
      "loss: 2.0864, acc: 0.8556\n",
      "loss: 2.0850, acc: 0.8557\n",
      "loss: 2.0835, acc: 0.8558\n",
      "loss: 2.0821, acc: 0.8559\n",
      "loss: 2.0807, acc: 0.8560\n",
      "loss: 2.0793, acc: 0.8561\n",
      "loss: 2.0779, acc: 0.8561\n",
      "loss: 2.0764, acc: 0.8562\n",
      "loss: 2.0750, acc: 0.8563\n",
      "loss: 2.0736, acc: 0.8564\n",
      "loss: 2.0722, acc: 0.8565\n",
      "loss: 2.0708, acc: 0.8566\n",
      "loss: 2.0703, acc: 0.8566\n",
      "val_report: {'precision': 0.3076923076923077, 'recall': 1.0, 'f1-score': 0.47058823529411764, 'support': 4}\n",
      "loss: 2.0689, acc: 0.8567\n",
      "loss: 2.0675, acc: 0.8568\n",
      "loss: 2.0662, acc: 0.8568\n",
      "loss: 2.0824, acc: 0.8568\n",
      "loss: 2.0811, acc: 0.8569\n",
      "loss: 2.0797, acc: 0.8569\n",
      "loss: 2.0783, acc: 0.8570\n",
      "loss: 2.0769, acc: 0.8571\n",
      "loss: 2.0755, acc: 0.8572\n",
      "loss: 2.0741, acc: 0.8573\n",
      "loss: 2.0727, acc: 0.8574\n",
      "loss: 2.0713, acc: 0.8575\n",
      "loss: 2.0699, acc: 0.8576\n",
      "loss: 2.0685, acc: 0.8577\n",
      "loss: 2.0671, acc: 0.8578\n",
      "loss: 2.0657, acc: 0.8579\n",
      "loss: 2.0643, acc: 0.8580\n",
      "loss: 2.0630, acc: 0.8581\n",
      "loss: 2.0616, acc: 0.8581\n",
      "loss: 2.0603, acc: 0.8582\n",
      "loss: 2.0589, acc: 0.8583\n",
      "loss: 2.0576, acc: 0.8584\n",
      "loss: 2.0563, acc: 0.8585\n",
      "loss: 2.0550, acc: 0.8585\n",
      "loss: 2.0536, acc: 0.8586\n",
      "loss: 2.0523, acc: 0.8587\n",
      "loss: 2.0509, acc: 0.8588\n",
      "loss: 2.0504, acc: 0.8588\n",
      "val_report: {'precision': 0.5714285714285714, 'recall': 1.0, 'f1-score': 0.7272727272727273, 'support': 4}\n",
      "loss: 2.0491, acc: 0.8589\n",
      "loss: 2.0477, acc: 0.8590\n",
      "loss: 2.0463, acc: 0.8591\n",
      "loss: 2.0450, acc: 0.8592\n",
      "loss: 2.0438, acc: 0.8592\n",
      "loss: 2.0426, acc: 0.8592\n",
      "loss: 2.0418, acc: 0.8593\n",
      "loss: 2.0406, acc: 0.8593\n",
      "loss: 2.0412, acc: 0.8591\n",
      "loss: 2.0410, acc: 0.8592\n",
      "loss: 2.0403, acc: 0.8592\n",
      "loss: 2.0399, acc: 0.8590\n",
      "loss: 2.0400, acc: 0.8590\n",
      "loss: 2.0390, acc: 0.8590\n",
      "loss: 2.0377, acc: 0.8591\n",
      "loss: 2.0364, acc: 0.8592\n",
      "loss: 2.0350, acc: 0.8593\n",
      "loss: 2.0337, acc: 0.8593\n",
      "loss: 2.0326, acc: 0.8593\n",
      "loss: 2.0320, acc: 0.8593\n",
      "loss: 2.0308, acc: 0.8593\n",
      "loss: 2.0295, acc: 0.8594\n",
      "loss: 2.0282, acc: 0.8595\n",
      "loss: 2.0268, acc: 0.8596\n",
      "loss: 2.0255, acc: 0.8597\n",
      "loss: 2.0242, acc: 0.8598\n",
      "loss: 2.0229, acc: 0.8599\n",
      "loss: 2.0225, acc: 0.8599\n",
      "val_report: {'precision': 0.3333333333333333, 'recall': 1.0, 'f1-score': 0.5, 'support': 4}\n",
      "loss: 2.0212, acc: 0.8600\n",
      "loss: 2.0198, acc: 0.8601\n",
      "loss: 2.0185, acc: 0.8602\n",
      "loss: 2.0173, acc: 0.8603\n",
      "loss: 2.0160, acc: 0.8604\n",
      "loss: 2.0147, acc: 0.8604\n",
      "loss: 2.0134, acc: 0.8605\n",
      "loss: 2.0122, acc: 0.8605\n",
      "loss: 2.0113, acc: 0.8605\n",
      "loss: 2.0100, acc: 0.8606\n",
      "loss: 2.0087, acc: 0.8607\n",
      "loss: 2.0075, acc: 0.8608\n",
      "loss: 2.0062, acc: 0.8609\n",
      "loss: 2.0049, acc: 0.8610\n",
      "loss: 2.0036, acc: 0.8611\n",
      "loss: 2.0023, acc: 0.8612\n",
      "loss: 2.0010, acc: 0.8612\n",
      "loss: 1.9997, acc: 0.8613\n",
      "loss: 1.9984, acc: 0.8614\n",
      "loss: 1.9971, acc: 0.8615\n",
      "loss: 1.9959, acc: 0.8616\n",
      "loss: 1.9946, acc: 0.8616\n",
      "loss: 1.9933, acc: 0.8617\n",
      "loss: 1.9921, acc: 0.8618\n",
      "loss: 1.9908, acc: 0.8619\n",
      "loss: 1.9896, acc: 0.8619\n",
      "loss: 1.9886, acc: 0.8618\n",
      "loss: 1.9886, acc: 0.8617\n",
      "val_report: {'precision': 0.25, 'recall': 1.0, 'f1-score': 0.4, 'support': 4}\n",
      "loss: 1.9873, acc: 0.8618\n",
      "loss: 1.9861, acc: 0.8619\n",
      "loss: 1.9849, acc: 0.8620\n",
      "loss: 1.9837, acc: 0.8620\n",
      "loss: 1.9824, acc: 0.8621\n",
      "loss: 1.9811, acc: 0.8622\n",
      "loss: 1.9799, acc: 0.8623\n",
      "loss: 1.9786, acc: 0.8623\n",
      "loss: 1.9774, acc: 0.8624\n",
      "loss: 1.9763, acc: 0.8625\n",
      "loss: 1.9751, acc: 0.8625\n",
      "loss: 1.9759, acc: 0.8623\n",
      "loss: 1.9764, acc: 0.8623\n",
      "loss: 1.9752, acc: 0.8624\n",
      "loss: 1.9739, acc: 0.8625\n",
      "loss: 1.9727, acc: 0.8626\n",
      "loss: 1.9715, acc: 0.8626\n",
      "loss: 1.9702, acc: 0.8627\n",
      "loss: 1.9690, acc: 0.8628\n",
      "loss: 1.9677, acc: 0.8629\n",
      "loss: 1.9664, acc: 0.8630\n",
      "loss: 1.9652, acc: 0.8631\n",
      "loss: 1.9639, acc: 0.8632\n",
      "loss: 1.9627, acc: 0.8632\n",
      "loss: 1.9614, acc: 0.8633\n",
      "loss: 1.9601, acc: 0.8634\n",
      "loss: 1.9589, acc: 0.8635\n",
      "loss: 1.9585, acc: 0.8635\n",
      "val_report: {'precision': 0.26666666666666666, 'recall': 1.0, 'f1-score': 0.4210526315789474, 'support': 4}\n",
      "loss: 1.9573, acc: 0.8636\n",
      "loss: 1.9560, acc: 0.8637\n",
      "loss: 1.9548, acc: 0.8638\n",
      "loss: 1.9536, acc: 0.8639\n",
      "loss: 1.9524, acc: 0.8639\n",
      "loss: 1.9511, acc: 0.8640\n",
      "loss: 1.9499, acc: 0.8641\n",
      "loss: 1.9487, acc: 0.8642\n",
      "loss: 1.9475, acc: 0.8642\n",
      "loss: 1.9470, acc: 0.8641\n",
      "loss: 1.9465, acc: 0.8641\n",
      "loss: 1.9455, acc: 0.8642\n",
      "loss: 1.9443, acc: 0.8642\n",
      "loss: 1.9431, acc: 0.8643\n",
      "loss: 1.9419, acc: 0.8644\n",
      "loss: 1.9406, acc: 0.8645\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.9394, acc: 0.8646\n",
      "loss: 1.9382, acc: 0.8647\n",
      "loss: 1.9370, acc: 0.8647\n",
      "loss: 1.9358, acc: 0.8648\n",
      "loss: 1.9346, acc: 0.8649\n",
      "loss: 1.9334, acc: 0.8650\n",
      "loss: 1.9322, acc: 0.8651\n",
      "loss: 1.9310, acc: 0.8651\n",
      "loss: 1.9298, acc: 0.8652\n",
      "loss: 1.9286, acc: 0.8653\n",
      "loss: 1.9274, acc: 0.8654\n",
      "loss: 1.9270, acc: 0.8654\n",
      "val_report: {'precision': 0.4444444444444444, 'recall': 1.0, 'f1-score': 0.6153846153846153, 'support': 4}\n",
      "loss: 1.9258, acc: 0.8655\n",
      "loss: 1.9246, acc: 0.8656\n",
      "loss: 1.9234, acc: 0.8657\n",
      "loss: 1.9222, acc: 0.8658\n",
      "loss: 1.9211, acc: 0.8658\n",
      "loss: 1.9204, acc: 0.8659\n",
      "loss: 1.9201, acc: 0.8658\n",
      "loss: 1.9196, acc: 0.8658\n",
      "loss: 1.9184, acc: 0.8659\n",
      "loss: 1.9172, acc: 0.8660\n",
      "loss: 1.9162, acc: 0.8660\n",
      "loss: 1.9166, acc: 0.8659\n",
      "loss: 1.9166, acc: 0.8659\n",
      "loss: 1.9155, acc: 0.8659\n",
      "loss: 1.9143, acc: 0.8660\n",
      "loss: 1.9132, acc: 0.8661\n",
      "loss: 1.9121, acc: 0.8661\n",
      "loss: 1.9112, acc: 0.8661\n",
      "loss: 1.9101, acc: 0.8662\n",
      "loss: 1.9093, acc: 0.8662\n",
      "loss: 1.9100, acc: 0.8660\n",
      "loss: 1.9097, acc: 0.8660\n",
      "loss: 1.9088, acc: 0.8661\n",
      "loss: 1.9076, acc: 0.8661\n",
      "loss: 1.9065, acc: 0.8662\n",
      "loss: 1.9053, acc: 0.8663\n",
      "loss: 1.9042, acc: 0.8663\n",
      "loss: 1.9038, acc: 0.8664\n",
      "val_report: {'precision': 0.25, 'recall': 1.0, 'f1-score': 0.4, 'support': 4}\n",
      "loss: 1.9026, acc: 0.8665\n",
      "loss: 1.9015, acc: 0.8665\n",
      "loss: 1.9003, acc: 0.8666\n",
      "loss: 1.8992, acc: 0.8667\n",
      "loss: 1.8980, acc: 0.8668\n",
      "loss: 1.8968, acc: 0.8669\n",
      "loss: 1.8957, acc: 0.8669\n",
      "loss: 1.8946, acc: 0.8670\n",
      "loss: 1.8934, acc: 0.8671\n",
      "loss: 1.8923, acc: 0.8672\n",
      "loss: 1.8911, acc: 0.8672\n",
      "loss: 1.8900, acc: 0.8673\n",
      "loss: 1.8888, acc: 0.8674\n",
      "loss: 1.8877, acc: 0.8675\n",
      "loss: 1.8865, acc: 0.8676\n",
      "loss: 1.8856, acc: 0.8676\n",
      "loss: 1.8862, acc: 0.8674\n",
      "loss: 1.8868, acc: 0.8674\n",
      "loss: 1.8857, acc: 0.8675\n",
      "loss: 1.8846, acc: 0.8675\n",
      "loss: 1.8834, acc: 0.8676\n",
      "loss: 1.8823, acc: 0.8677\n",
      "loss: 1.8812, acc: 0.8678\n",
      "loss: 1.8800, acc: 0.8679\n",
      "loss: 1.8789, acc: 0.8679\n",
      "loss: 1.8778, acc: 0.8680\n",
      "loss: 1.8766, acc: 0.8681\n",
      "loss: 1.8763, acc: 0.8681\n",
      "val_report: {'precision': 0.3076923076923077, 'recall': 1.0, 'f1-score': 0.47058823529411764, 'support': 4}\n",
      "loss: 1.8751, acc: 0.8682\n",
      "loss: 1.8741, acc: 0.8683\n",
      "loss: 1.8729, acc: 0.8684\n",
      "loss: 1.8718, acc: 0.8684\n",
      "loss: 1.8707, acc: 0.8685\n",
      "loss: 1.8696, acc: 0.8686\n",
      "loss: 1.8685, acc: 0.8687\n",
      "loss: 1.8674, acc: 0.8687\n",
      "loss: 1.8663, acc: 0.8688\n",
      "loss: 1.8651, acc: 0.8689\n",
      "loss: 1.8640, acc: 0.8690\n",
      "loss: 1.8629, acc: 0.8690\n",
      "loss: 1.8618, acc: 0.8691\n",
      "loss: 1.8607, acc: 0.8692\n",
      "loss: 1.8604, acc: 0.8691\n",
      "loss: 1.8598, acc: 0.8691\n",
      "loss: 1.8590, acc: 0.8691\n",
      "loss: 1.8579, acc: 0.8692\n",
      "loss: 1.8568, acc: 0.8693\n",
      "loss: 1.8558, acc: 0.8693\n",
      "loss: 1.8547, acc: 0.8694\n",
      "loss: 1.8536, acc: 0.8694\n",
      "loss: 1.8525, acc: 0.8695\n",
      "loss: 1.8514, acc: 0.8696\n",
      "loss: 1.8503, acc: 0.8697\n",
      "loss: 1.8492, acc: 0.8698\n",
      "loss: 1.8481, acc: 0.8698\n",
      "loss: 1.8478, acc: 0.8699\n",
      "val_report: {'precision': 0.3076923076923077, 'recall': 1.0, 'f1-score': 0.47058823529411764, 'support': 4}\n",
      "loss: 1.8467, acc: 0.8699\n",
      "loss: 1.8456, acc: 0.8700\n",
      "loss: 1.8445, acc: 0.8701\n",
      "loss: 1.8434, acc: 0.8702\n",
      "loss: 1.8424, acc: 0.8702\n",
      "loss: 1.8432, acc: 0.8700\n",
      "loss: 1.8433, acc: 0.8700\n",
      "loss: 1.8422, acc: 0.8701\n",
      "loss: 1.8411, acc: 0.8702\n",
      "loss: 1.8400, acc: 0.8702\n",
      "loss: 1.8389, acc: 0.8703\n",
      "loss: 1.8378, acc: 0.8704\n",
      "loss: 1.8367, acc: 0.8705\n",
      "loss: 1.8357, acc: 0.8705\n",
      "loss: 1.8347, acc: 0.8706\n",
      "loss: 1.8336, acc: 0.8707\n",
      "loss: 1.8325, acc: 0.8707\n",
      "loss: 1.8315, acc: 0.8708\n",
      "loss: 1.8304, acc: 0.8709\n",
      "loss: 1.8293, acc: 0.8710\n",
      "loss: 1.8283, acc: 0.8710\n",
      "loss: 1.8272, acc: 0.8711\n",
      "loss: 1.8262, acc: 0.8712\n",
      "loss: 1.8251, acc: 0.8712\n",
      "loss: 1.8241, acc: 0.8713\n",
      "loss: 1.8233, acc: 0.8713\n",
      "loss: 1.8222, acc: 0.8714\n",
      "loss: 1.8219, acc: 0.8714\n",
      "val_report: {'precision': 0.3333333333333333, 'recall': 1.0, 'f1-score': 0.5, 'support': 4}\n",
      "{'precision': 0.3333333333333333, 'recall': 0.125, 'f1-score': 0.18181818181818182, 'support': 8}\n"
     ]
    }
   ],
   "source": [
    "seed = 1253\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "optimizers = {\n",
    "    'adadelta': torch.optim.Adadelta,  # default lr=1.0\n",
    "    'adagrad': torch.optim.Adagrad,  # default lr=0.01\n",
    "    'adam': torch.optim.Adam,  # default lr=0.001\n",
    "    'adamax': torch.optim.Adamax,  # default lr=0.002\n",
    "    'asgd': torch.optim.ASGD,  # default lr=0.01\n",
    "    'rmsprop': torch.optim.RMSprop,  # default lr=0.01\n",
    "    'sgd': torch.optim.SGD,\n",
    "    'adamw':torch.optim.AdamW,\n",
    "}\n",
    "\"\"\"You should test different Hidden dimension, and learning rate to find the best model\"\"\"\n",
    "N, D_in, H, D_out = 64, 18, 30, 2\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ")\n",
    "learning_rate=0.7\n",
    "_params = filter(lambda p: p.requires_grad, model.parameters())\n",
    "opt_rec = 'adadelta'\n",
    "optimizer = optimizers[opt_rec](_params, lr=learning_rate, weight_decay=0.01)\n",
    "#===================================================================================#\n",
    "num_epoch = 60\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "lrshrink = 5\n",
    "\n",
    "global_step = 0\n",
    "max_val_f1 = 0\n",
    "n_correct, n_total, loss_total = 0, 0, 0\n",
    "t_targets_all, t_outputs_all = None, None\n",
    "#===================================================================================#\n",
    "_reset_params(model, 'uniform')\n",
    "stop_training = False\n",
    "adam_stop = False\n",
    "mimicloss=[]\n",
    "epoch = 0\n",
    "\n",
    "while not stop_training and epoch <= num_epoch:\n",
    "    model.train()\n",
    "    for i_batch, sample_batched in enumerate(train_x):        \n",
    "        outputs = model(sample_batched['features'])\n",
    "        targets = sample_batched['class_n']\n",
    "        loss = loss_fn(outputs, targets)     \n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if t_targets_all is None:\n",
    "            t_targets_all = targets\n",
    "            t_outputs_all = outputs\n",
    "        else:\n",
    "            t_targets_all = torch.cat((t_targets_all, targets), dim=0)\n",
    "            t_outputs_all = torch.cat((t_outputs_all, outputs), dim=0)\n",
    "\n",
    "#         report = classification_report(t_targets_all.cpu(), torch.argmax(t_outputs_all, -1).cpu(), target_names=['class0','class1'], output_dict=True, digits = 4)\n",
    "\n",
    "        n_correct += (torch.argmax(outputs, -1) == targets).sum().item()\n",
    "        n_total += len(outputs)\n",
    "        loss_total += loss.item() * len(outputs)\n",
    "        if global_step % 2 == 0:\n",
    "            train_acc = n_correct / n_total\n",
    "            train_loss = loss_total / n_total\n",
    "            mimicloss.append(train_loss)\n",
    "            print('loss: {:.4f}, acc: {:.4f}'.format(train_loss, train_acc))\n",
    "\n",
    "    val_acc, val_f1, report_val = _evaluate_acc_f1(val_x, model)\n",
    "    print('val_report: {}'.format(report_val['class1']))\n",
    "    epoch +=1\n",
    "\n",
    "\n",
    "    if report_val['class1']['f1-score'] > max_val_f1:\n",
    "        max_val_f1 = report_val['class1']['f1-score']\n",
    "        print('\\nSaving model')\n",
    "        if not os.path.exists('state_dict/'+'imdb'):\n",
    "            os.mkdir('state_dict/'+'imdb')\n",
    "        path = 'state_dict/'+'imdb'+'/{0}_{1}'.format('neuralNet', 'imdb')\n",
    "        torch.save(model.state_dict(), path)\n",
    "    else:\n",
    "        if 'sgd' in opt_rec:\n",
    "            optimizer.param_groups[0]['lr'] = optimizer.param_groups[0]['lr'] / lrshrink\n",
    "            print('Shrinking lr by : {0}. New lr = {1}'\n",
    "                  .format(lrshrink,\n",
    "                          optimizer.param_groups[0]['lr']))\n",
    "            if optimizer.param_groups[0]['lr'] < 1e-5:\n",
    "                stop_training = True\n",
    "        if 'adam' in opt_rec:\n",
    "                    # early stopping (at 2nd decrease in accuracy)\n",
    "            stop_training = adam_stop\n",
    "            adam_stop = True\n",
    "\n",
    "\n",
    "\n",
    "#predict on test\n",
    "best_path = path\n",
    "\n",
    "model.load_state_dict(torch.load(best_path,map_location=torch.device('cpu')))\n",
    "model.eval()\n",
    "test_acc, test_f1, report_test  = _evaluate_acc_f1(test_x, model)\n",
    "print(report_test['class1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
